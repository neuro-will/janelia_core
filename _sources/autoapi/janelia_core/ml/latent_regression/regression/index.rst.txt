:py:mod:`janelia_core.ml.latent_regression.regression`
======================================================

.. py:module:: janelia_core.ml.latent_regression.regression

.. autoapi-nested-parse::

   Tools for working with latent regression models where we use point estimates of modes instead of distributions.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   janelia_core.ml.latent_regression.regression.SubjectRegressionCollection
   janelia_core.ml.latent_regression.regression.MultiSubjectRegressionFitter



Functions
~~~~~~~~~

.. autoapisummary::

   janelia_core.ml.latent_regression.regression.format_output_list
   janelia_core.ml.latent_regression.regression.predict



.. py:function:: format_output_list(base_str: str, it_str: str, vls: Sequence[float], inds: Sequence[int])

   Produces a string to display a list of outputs.

   String will be of the format:

       base_str + ' ' + it_str+ints[0] + ': ' + str(vls[0]) + ', ' + it_str+inds[1] + ': ' + str(vls[1]) + ...

   Args:
       base_str: The base string that preceeds everything else on the string

       it_str: The string that should come befor each printed value

       vls: The values that should be printed

       inds: The indices to associate with each printed value

   Returns:
       f_str: The formatted string


.. py:class:: SubjectRegressionCollection(s_mdl: janelia_core.ml.latent_regression.subject_models.LatentRegModel, data: janelia_core.ml.datasets.TimeSeriesBatch, input_grps: Sequence, output_grps: Sequence, props: Sequence[torch.Tensor], input_props: Sequence[int], output_props: Sequence[int], p_preds: Sequence[Union[torch.nn.Module, torch.Tensor]], u_preds: Sequence[Union[torch.nn.Module, torch.Tensor]])

   An object which holds subject data, properties and likelihood model.



   Creates a new SubjectVICollection object.

   Args:
       s_mdl: The likelihood model for the subject.

       data: Data for the subject.

       input_grps: input_grps[g] is the index into data.data for the g^th input group

       output_grps: output_grps[h] is the index into data.data for the h^th output group

       props: props[i] is a tensor of properties for one or more input or output groups of variables

       input_props: input_props[g] is the index into props for the properties for the g^th input group.  If
       the modes for the g^th input group are fixed, then input_props[g] should be None.

       output_props: output_props[h[ is the index into props for the properties of the g^th output group.  If
       the modes for the h^th output group are fixed, then outpout_props[h] should be None.

       p_preds: p_pred[g] is the predictor to use for the modes of the g^th input group.  If the modes for this
       input group are fixed, then g should be a tensor with the fixed values.

       u_preds: u_preds are the predictors for the modes for the output groups, same format as p_preds.


   .. py:method:: trainable_parameters(self) -> Sequence

      Returns all trainable parameters for the collection.

      Returns:
          params: The list of parameters.


   .. py:method:: to(self, device: Union[torch.device, int], distribute_data: bool = False)

      Moves all relevant attributes of the collection to the specified device.

      This will move the subject model and properties to the device as well as predictors.

      Note that by default fitting data will not be moved to the device.

      Args:
          device: The device to move attributes to.

          distribute_data: True if fitting data should be moved to the device as well



.. py:class:: MultiSubjectRegressionFitter(s_collections: Sequence[SubjectRegressionCollection], min_var: float = 0.001)

   Object for fitting a collection of latent regression models with point estimates for modes.



   Creates a new MultiSubjectRegressionFitter object.

   Args:
       s_collections: A set of SubjectRegressionCollections to use when fitting data.

       min_var: The minimum variance to enforce on the final noise distributions of output variables.


   .. py:method:: create_check_point(self, copy_data: bool = False, copy_props: bool = False, subj_specific_preds: bool = False)

      Creates a check point of everything in the subject collections.

      By default, data and properties will not be copied for each subject.

      Be default, the check point assumes the predictors for each subject are the same.  In this case, to save
      memory, instead of making separate copies of the predictors for each subject, all subject collections will
      reference the same set of predictors. If this is not the case, the user can specify that subjects have
      their own predictors, (see subj_specific_preds input below) in which case separate copies of the predictors
      in each subject collection will be created.

      Args:
          copy_data: True if data should be included in each subject collection.

          copy_props: True if properties should be included in each subject collection.

          subj_specific_preds: False if all subjects use the same predictors.  True if subjects have their own
          unique predictors.

      Returns:
          check_point: A dictionary, currently with a single key 's_collections' that contains copies
          of the s_collections.


   .. py:method:: distribute(self, devices: Sequence[Union[torch.device, int]], s_inds: Sequence[int] = None, distribute_data: bool = False)

      Distributes subject regression collections across devices.

      Args:
          devices: Devices that things can be distributed across.

          s_inds: Indices into self.s_collections for subject regression collections which should be distributed
          across devices.  If None, collections for all subjects will be distributed.

          distribute_data: True if all training data should be distributed to devices.  If there is enough
          device memory, this can speed up fitting.  If not, set this to false, and batches of data will
          be sent to the device for each training iteration.



   .. py:method:: trainable_parameters(self, s_inds: Sequence[int] = None) -> list

      Gets all trainable parameters for fitting predictors and a set of subjects.

      Args:
          s_inds: Specifies the indices of subjects that will be fit.  Subject indices correspond to their
          original order in s_collections when the fitter was created. If None, all subjects used.

      Returns:
           params: Parameters of mode predictors and subject models


   .. py:method:: generate_batch_smp_inds(self, n_batches: int, s_inds: Sequence[int] = None)

      Generates indices of random mini-batches of samples for each subject.

      Args:
          n_batches: The number of batches to break the data up for each subject into.

          s_inds: Specifies the indices of subjects that will be fit.  Subject indices correspond to their
          original order in s_collections when the fitter was created. If None, s_inds = range(n_subjects).

      Returns:
          batch_smp_inds: batch_smp_inds[i][j] is the sample indices for the j^th batch for subject s_inds[i]



   .. py:method:: get_used_devices(self)

      Lists the devices the subject models and predictors are on.

      Returns:
          devices: The list of devices subject models and priors are on.


   .. py:method:: fit(self, n_epochs: int = 10, n_batches: int = 10, learning_rates=0.01, adam_params: dict = {}, s_inds: Sequence[int] = None, update_int: int = 1, print_mdl_nlls: bool = True, print_memory_usage: bool = True, cp_epochs: Sequence[int] = None, cp_opts: dict = None) -> dict

      Args:

          n_epochs: The number of epochs to run fitting for.

          n_batches: The number of batches to break the training data up into per epoch.  When multiple subjects have
          different numbers of total training samples, the batch size for each subject will be selected so we go
          through the entire training set for each subject after processing n_batches each epoch.

          learning_rates: If a single number, this is the learning rate to use for all epochs and parameters.
          Alternatively, this can be a list of tuples.  Each tuple is of the form
          (epoch,lr), where epoch is the epoch the learning rates come into effect on, lr is the learning rate.
          Multiple tuples can be provided to give a schedule of learning rates.  Here is an example
          learning_rates: [(0, .001), (100, .0001)] that starts with a learning rates of .001, and epoch 100, the
          learning rate goes to .0001.

          adam_params: Dictionary of parameters to pass to the call when creating the Adam Optimizer object.
          Note that *if learning rate is specified here it will be ignored.* (Use the learning_rates option instead).
          The options specified here will be applied to all parameters at all iterations.

          s_inds: Specifies the indices of subjects to fit to.  Subject indices correspond to their
          original order in s_collections when the fitter was created. If None, all subjects used.

          update_int: Fitting status will be printed to screen every update_int number of epochs

          print_mdl_nlls: If true, when fitting status is printed to screen, the negative log likelihood of each
          evaluated model will be printed to screen.

          print_memory_usage: If true, when fitting status is printed to screen, the memory usage of each
          device will be printed to streen.

          cp_epochs: A sequence of epochs after which a check point of the models will be made.  If no check points
          should be made, set this to None.

          cp_opts: A dictionary with options that can be passed into self.make_check_point when making check points.
          If None, default options are used.

      Return:
          log: A dictionary with the following entries:

              'elapsed_time': A numpy array of the elapsed time for completion of each epoch

              'mdl_nll': mdl_nll[e, i] is the negative log likelihood for the subject model s_inds[i] at the start
              of epoch e (that is when the objective has been calculated but before parameters have been updated)

              obj: obj[e] contains the objective value at the start of epoch e.  This is the negative evidence lower
              bound + weight penalties.

          checkpoints: None if no check points created.  Otherwise, checkpoints[i] is the check point created
          at epoch cp_epochs[i].

      Raises:
          RuntimeError: If distribute() has not been called before fitting.



   .. py:method:: plot_log(cls, log: dict)
      :classmethod:

      Produces a figure of the values in a log produced by fit().

      Args:
          log: The log to plot.



.. py:function:: predict(s_collection: SubjectRegressionCollection, data: janelia_core.ml.datasets.TimeSeriesBatch, batch_size: int = 100) -> List[numpy.ndarray]

   Predicts output given input from a model with predictions over modes.

   Note: All predictions will be returned on host (cpu) memory as numpy arrays.

   Args:
       s_collection: The collection for the subject.   Any data in the collection will be ignored.

       data: The data to predict with.

       batch_size: The number of samples we predict on at a time.  This is helpful if using a GPU
       with limited memory.

   Returns:
       pred_mn: The predicted means given the input.


