:py:mod:`janelia_core.ml.latent_regression.scenarios`
=====================================================

.. py:module:: janelia_core.ml.latent_regression.scenarios

.. autoapi-nested-parse::

   Tools for generating simulated latent regression models.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   janelia_core.ml.latent_regression.scenarios.GaussianBumpFcn
   janelia_core.ml.latent_regression.scenarios.ConstantFcn
   janelia_core.ml.latent_regression.scenarios.IdentityS
   janelia_core.ml.latent_regression.scenarios.BumpInputWithRecursiveDynamicsScenario
   janelia_core.ml.latent_regression.scenarios.SplitPropertiesScenario



Functions
~~~~~~~~~

.. autoapisummary::

   janelia_core.ml.latent_regression.scenarios.plot_single_2d_conditional_prior
   janelia_core.ml.latent_regression.scenarios.plot_2d_conditional_prior
   janelia_core.ml.latent_regression.scenarios.plot_single_2d_mode
   janelia_core.ml.latent_regression.scenarios.plot_single_2d_mode_posterior
   janelia_core.ml.latent_regression.scenarios.plot_2d_modes
   janelia_core.ml.latent_regression.scenarios.learn_prior_transformation



.. py:class:: GaussianBumpFcn(ctr: torch.Tensor, std: torch.Tensor, peak_vl: float)

   Bases: :py:obj:`torch.nn.Module`

   A Gaussian bump function with fixed parameters.

   The bump function will be axis-aligned but can have different standard deviations along each axis.

   Creates a new GaussianBump module.

   Args:
       ctr: A 1-d tensor giving the center location of the bump.

       std: A 1-d tensor giving the standard deviation of the bump along each axis.

       peak_vl: The value at the peak of the bump.

   .. py:method:: forward(self, x: torch.Tensor)

      Computes input from output.

      Args:
          x: Input.  Each row is a sample.

      Returns:
          y: Output.  Each row corresponds to a transformed input sample.



.. py:class:: ConstantFcn(c: torch.Tensor)

   Bases: :py:obj:`torch.nn.Module`

   Represents a function which is constant everywhere with fixed parameters.

   Creates a ConstantFcn object.

   Args:
       c: Constant value of function.  A 1-d array with length equal to dimensionality of output.

   .. py:method:: forward(self, x: torch.Tensor) -> torch.Tensor

      Computes output from input.

      Args:
          x: Input of shape n_smps*d_x

          y: Output of shape n_smps*d_y.  (Each row will be equal to the constant value of the function.)



.. py:class:: IdentityS

   Bases: :py:obj:`torch.nn.Module`

   Module which just passes through input.

   Initializes internal Module state, shared by both nn.Module and ScriptModule.

   .. py:method:: forward(self, x)



.. py:class:: BumpInputWithRecursiveDynamicsScenario(n_subjects, n_modes: int, n_neuron_range: Sequence[int], prior_std: float = 0.02, bump_max_range: Sequence[float] = [0.09, 0.2], bump_std_range: Sequence[int] = [0.2, 0.4], noise_range: Sequence[float] = [0.1, 0.2], n_dims: int = 2, pos_neg_modes: bool = True, input_bump_ctrs: numpy.ndarray = None, input_bump_stds: numpy.ndarray = None, input_bump_gains: numpy.ndarray = None, int_bump_ctrs: numpy.ndarray = None, int_bump_stds: numpy.ndarray = None, int_bump_gains: numpy.ndarray = None)

   Objects for generating models and data where we record neurons from multiple subjects receiving stimuli.

   We simulate recording neural activity from multiple subjects while they are presented stimuli.  There are a set of
   inputs that drive different patterns of activity across the population.  Activity across the population is then read
   out and in turn drive other sets of patterns (internal dynamics).

   Neurons are characterized by their position in a unit hypercube (number of dimensions is user set).

   The patterns of activity that is driven be each stimulus are determined by the loadings of a set of modes, one mode
   for each stimulus.  The loadings for neurons for each mode is a probabilistic function of their position.
   Specifically, we select loadings for modes for each subject from bump shaped priors.  Similarly, the modes which
   determine the internal dynamics of the population also come from bump shaped priors.

   By "bump shaped prior" we mean that the conditional distribution for a neuron's loading in a particular mode
   given it's position in space is Gaussian with a conditional mean which is shaped like a bump in space. In this
   simple scenario, we assume the standard deviation of the conditional distributions is the same irrespective of a
   neuron's location in space.

   We select the p modes of the internal dynamics to be roughly aligned with the u modes of the stimuli, to ensure
   that stimulus activity is propagated into the internal dynamics of the population.

   The logic for using this object is as follows:

       1) Initialize a scenario - this creates the priors and the subject models

       2) Simulate data for the scenario using the generate_data function


   Creates a new BumpInputWithRecursiveDynamicsScenario object.

   Args:
       n_subjects: The number of subject models to generate.

       n_modes: The number of modes each model should have.  There will be 2*n_modes p and u modes per subject.

       n_neuron_range: The number of neurons each subject has will be pulled uniformly from this range.

       prior_std: The standard deviation of each prior conditional distribution.

       bump_gain_range: We use a Gaussian kernel for the bump functions which form the conditional means for each
       mode. Each bump has a randomly chosen peak value magnitude.  These peak values are pulled uniformly from the
       interval specified by bump_gain_range. See pos_neg_modes below about how the signs of these peaks are
       chosen.

       bump_std_range: The Gaussian kernel functions for the conditional means will be axis aligned with a certain
       standard deviation along each axis.  The standard deviation values along each axis will be pulled uniformly
       from this range.

       noise_range: Range of values to pull psi values from when generating latent regression models

       n_dims: The number of spatial dimensions neurons are arranged in.

       pos_neg_modes: If true, the sign of the peak of modes can be either positive or negative (with 50%
       probability)

       input_bump_ctrs: If not None, the parameters of the bump function forming the means for the input mode
       couplings (and internal p couplings) will not be randomly generated but will instead be specified by the
       user.  input_bump_ctrs[:, i] gives the center for the bump function for the i^th mode.

       input_bump_stds: If not randomly generating parameters for the input mean functions, input_bump_stds[:, i]
       gives the dimension standard deviations for the i^th mode.

       input_bump_gains: If not randomly generating parameters for the input mean functions, input_bump_gains[i]
       gives the gain for the i^th mode.

       int_bump_ctrs, int_bump_stds, int_bump_gains: Analagous to the same for the input modes but specifying the
       parameters of the mean functions for the internal u modes.


   .. py:method:: generate_training_subject_mdl(self, s_i: int, assign_p_u: bool = True) -> janelia_core.ml.latent_regression.subject_models.LatentRegModel

      Generates a new subject model with random parameters which can be fit to
      data generated from the scenario.

      Args:
          s_i: The subject to generate a model for

          assign_p_u: True if p and u tensors should be created for the model.  Setting this to false
          saves memory if the model will be fit with priors over the modes (in which case the p and u
          modes in the subject model object are ignored.)

      Returns:
          mdl: The generated subject model



   .. py:method:: generate_training_collection(self, s_i: int, data: janelia_core.ml.datasets.TimeSeriesDataset, mn_init_mn: float = 0, mn_init_std: float = 0.01, std_init_vl: float = 0.01, post_dists: list = None) -> janelia_core.ml.latent_regression.vi.SubjectVICollection

      Generates a SubjectVICollection for fitting data to a given subject in the scenario.

      Args:
          s_i: The index of the subject to fit to.

          data: The training data for the subject, as a TimeSeriesDataset in the same convention as returned by
          generate_data()

          mn_init_mn, mn_init_std, std_init_vl: Values for initializing distributions on modes.  See
          generate_training_subject_posteriors for more information.

          post_dists: If provided, these are the posterior distributions to use for this subject.  If not,
          posterior distributions will be generated by self.generate_training_subject_posteriors. If provided,
          post_dists[0] are the distributions for the p modes and post_dists[1] are the distributions for

      Returns:
          collection: The generated collection.


   .. py:method:: generate_shared_posteriors(self, n_divisions_per_dim: int = 50, n_div_per_hc_side_per_dim: int = 3, init_std: float = 0.001, min_std=1e-05) -> list

      Generates a shared posterior for fitting across models.

      Posteriors are based on hypercube mean and standard deviation functions.

      Args:
          n_divisions_per_dim: The number of divisons per dimension to use when generating hypercube functions.  There
          will be the same number of divisions for each dimension.

          n_div_per_hc_side_per_dim: The number of divisions per hypercube per dimension to use when generating
          hypercube functions.  This will be the same for all dimensions.

          init_std: The initial standard deviation for the conditional distribution for each mode.  The standard
          deviation will take on this value everywhere.

          min_std: The minimum standard deviation the posterior distributions can take on.

      Note: Because input p modes are assumed fixed and know, a posterior is not generated for these nodes.
      Instead a fixed tensor will be generated.

      Returns:
          dists: dists[0] are the posterior distributions for the p modes.  dists[0][0] is a tensor for the input
          p modes and dists[0][1] is a conditional distribution for the internal p modes. dists[1][0] is a
          conditional distribution for the u modes.


   .. py:method:: generate_training_subject_posteriors(self, s_i: int, mn_init_mn: float = 0, mn_init_std: float = 0.01, std_init_vl: float = 0.01) -> Sequence

      Generates the posterior distributions for a given subject for variational inference.

      Because the input p modes are assumed fixed and known, a tensor is returned in place of a distribution
      for these modes.

      Args:
          s_i: The subject to generate posteriors for

          mn_init_mn, mn_init_std: The mean and standard deviation of the normal distribution the initial mean values
          for each mode distribution are sampled from.

          std_init_vl: The initial standard deviation for each mode distribution.  The standard deviation of each
          conditional distribution is initially the same for all neuron positions.

      Returns:
          dists: dists[0] are the posterior distributions for the p modes.  dists[0][0] is a tensor for the input
          p modes and dists[0][1] is a conditional distribution for the internal p modes. dists[1][0] is a
          conditional distribution for the u modes.


   .. py:method:: generate_fitting_priors(self, n_divisions_per_dim: int = 50, n_div_per_hc_side_per_dim: int = 3, init_std: float = 0.001, min_std=1e-05) -> Sequence

      Generates priors for fitting multiple models with variational inference.

      Note: Because the input p modes are assumed fixed and known, a prior is not generate for these
      modes and the value None is returned in its place (see below).

      Args:
          n_divisions_per_dim: The number of divisons per dimension to use when generating hypercube functions.  There
          will be the same number of divisions for each dimension.

          n_div_per_hc_side_per_dim: The number of divisions per hypercube per dimension to use when generating
          hypercube functions.  This will be the same for all dimensions.

          init_std: The initial standard deviation for the conditional distribution for each mode.  The standard
          deviation will take on this value everywhere.

          min_std: The minimum standard deviation the prior distributions can take on.

      Returns:
          dists: dists[0] is the distributions for the p modes.  dists[0][0] is None to indicate there is no
          prior distribution over the input p modes and dists[0][1] is the distribution over the internal p modes.
          dists[1][0] is the distribution over the u modes.

      Raises:
          ValueError: If min_std or init_std is less than 0
          ValueError: If init_std is less than min_std


   .. py:method:: generate_data(self, stimuli: Sequence[torch.Tensor]) -> Sequence[Sequence]

      Generates data from subject models given time series of input to each model.

      Args:
          stimuli: stimuli[i] contains the input tensor for subject model i.

      Returns:
          data: data[i] contains data for subject i as a TimeSeriesDataset. The first tensor in
          each dataset will be input stimulus and the second will be neural data.  Data are
          "time locked" that is the t^th point in the returned stimulus and the t^th point in
          the returned neural data is the data that generated the (t+1)^th data point in the neural
          data.  This requires discarding the first point in the provided stimulus, so there will
          only by T-1 data points in the returned dataset.


   .. py:method:: generate_random_input_data(self, n_smps_per_subject: Sequence[int], input_std: float = 1.0) -> Sequence[Sequence]

      Generates data from each subject model as they receive random input.

      Data for each input is generated iid from a N(0, input_std^2) distribution.

      Args:
          n_smps_per_subject: n_smps_per_subject[i] is the number of random input samples to generate from subject
          i.

          input_std: The standard deviation of the input signals.

      Returns:
         data: data[i] is the data for subject i, the formatted described in generate_data()


   .. py:method:: generate_one_input_mode_random_input_data(self, n_smps_per_subject: Sequence[int], input_std: float = 1.0) -> Sequence[Sequence]

      Generates data where only one input mode is excited with random input per subject.

      Args:
          n_smps_per_subject: n_smps_per_subject[i] is the number of random input samples to generate from subject
          i.

          input_std: The standard deviation of the input signals.

      Returns:
         data: data[i] is the data for subject i, the formatted described in generate_data()



.. py:class:: SplitPropertiesScenario(n_neurons: Sequence[int], bump_k: float, bump_centers: Sequence[numpy.ndarray], bump_stds: Sequence[numpy.ndarray], coupling_std: float, psi_noise_range: Sequence[float])

   An object for simulating a scenario where neuron couplings depend on two or more sets of properties.

   For simplicity, we simulate neuron properties as distributed uniformly in a unit hypercube in each property space.

   Other than having neuron properties depend on two or more sets of properties, we try to keep everything
   else in this scenario as simple as possible.  In particular, we simulate a model of neurons driving one
   behavioral variable.  This means we have one p-mode per subject and fixed (scalar) u mode with value 1.  The
   coefficients of the p-mode depend on two sets of properties.  We assume identiy m-module and s-modules.

   To generating a p-mode coupling for a given subject's neuron , we pull from
   a N(m(p_i^0, p_i^1, ..., p_i^J), \sigma^2) distribution where m() is of the form:

       m(p_i^0, p_i^1, ..., p_i^J) = k*\prod_j^J b_j(p_i^j),

   where b_j is an axis-aligned Gaussian bump function parameterized by a center and standard deviation along
   each axis.


   Creates a new SplitPropertiesScenario object.

   Args:

       n_neurons: The length of n_neurons defines the number of subjects in the simulation. n_neurons[i]
       is the number of neurons to simulate for subject i.

       bump_k: The extreme value any coupling can take on.

       bump_centers: bump_centers[j] is where b_j is centered.  The length of bump_centers defines how many
       sets of properties there, and the length of bump_centers[j] implicitly defines the dimensionality of the
       j^th property space.

       bump_stds: Bump functions are axis aligned.  bump_stds[j][i] is the standard deviation of b_j along
       dimension i.

       coupling_std: The standard deviation of the distrubtion on couplings

       psi_noise_range: The range to pull psi values from when generating the latent regression model for
       each subject


   .. py:method:: _bmp_fcn(self, x_j, j)


   .. py:method:: _m(self, x)


   .. py:method:: generate_data(self, n_tm_pts: Sequence[int]) -> List[janelia_core.ml.datasets.TimeSeriesDataset]

      Generates simulated data for the scenario.

      Args:
          n_tm_pts: n_tm_pts[i] is the number of time points to generate for subject i

      Returns:
          data: data[i] is the data for subject i as a TimeSeriesDataset object.  The first entry in the .data
          attribute of the object will be neural data and the second will be behavioral data.


   .. py:method:: generate_training_subject_mdl(self, s_i: int, assign_p_u: bool = True, psi_noise_range: Sequence[float] = [0.1, 0.2]) -> janelia_core.ml.latent_regression.subject_models.LatentRegModel

      Generates a new subject model for training.

      Args:
          s_i: The subject to generate a model for

          assign_p_u: True if p and u tensors should be created for the model.  Setting this to false
          saves memory if the model will be fit with priors over the modes (in which case the p and u
          modes in the subject model object are ignored.)

          psi_noise_range: The range of values to initialize psi with

          Returns:
              mdl: The generated subject model


   .. py:method:: generate_training_subject_posteriors(self, s_i: int, mn_init_mn: float = 0, mn_init_std: float = 0.01, std_init_vl: float = 0.01) -> Sequence

      Generates the posterior distributions for a given subject for variational inference.

      Because the u mode is assumed fixed and known, a tensor is returned in place of a distribution
      for this mode.

      Args:
          s_i: The subject to generate posteriors for

          mn_init_mn, mn_init_std: The mean and standard deviation of the normal distribution the initial values
          for the mean of the distribution on the p mode are sampled from.

          std_init_vl: The initial standard deviation for each entry in the p-mode.  The standard deviation is
          initially the same for all neurons.

      Returns:
          p_dists: p_dists[0] is the distribtion on the p modes

          u_dists: u_dists[0] is a 1*1 tensor with value 1.



   .. py:method:: generate_training_collection(self, s_i: int, data: janelia_core.ml.datasets.TimeSeriesDataset, mn_init_mn: float = 0, mn_init_std: float = 0.01, std_init_vl: float = 0.01, post_dists: list = None) -> janelia_core.ml.latent_regression.vi.SubjectVICollection

      Generates a SubjectVICollection for fitting data to a given subject in the scenario.

      Args:
          s_i: The index of the subject to fit to.

          data: The training data for the subject, as a TimeSeriesDataset in the same convention as returned by
          generate_data()

          mn_init_mn, mn_init_std, std_init_vl: Values for initializing distributions on modes.  See
          generate_training_subject_posteriors for more information.

          post_dists: If provided, these are the posterior distributions to use for this subject.  If not,
          posterior distributions will be generated by self.generate_training_subject_posteriors. If provided,
          post_dists[0] are the distributions for the p modes and post_dists[1] are the distributions for

      Returns:
          collection: The generated collection.


   .. py:method:: generate_fitting_priors(self, set_inds: Sequence[Sequence], n_divisions_per_dim: int = 50, n_div_per_hc_side_per_dim: int = 3, init_mn: float = 0.01, init_std: float = 0.001, min_std=1e-05) -> Sequence

      Generates prior for fitting multiple models with variational inference.

      The only mode to generate a prior for in this scenario is the single p mode.  For neuron i with properties
      p_i^0, ... p_i^J, the prior conditioned on properties will be N(m(p_i^0, ... p_i^J), s^2(p_i^0, ... p_i^J)),
      where m() is of the form:

          m(p_i^0, ... p_i^J) = l*exp(\sum_{j=1}^J h_j^m(p_i^j) + d) + o,

      where each h_j^m is a hypercube function, d is a shift, o is an offset and l is a scale.

      The function s() is of the form:

          s(p_i^0, ... p_i^J) = exp(\sum_{j=1}^J h_j^s(p_i^j)) + o_fixed,

      where each h_j^s is again a hypercube function and o_fixed is a non-learnable parameter with a small
      positive value to ensure the output of s() is strictly positive.

      The priors will assume all of the properties have been concatenated together, and therefore, the
      user has to specify which dimensions of the concatenated properties belong to each set (see set_inds below).

      Note: Because u mode is assumed fixed and known, a prior is not generated for this mode
      and the value None is returned in its place (see below).

      Args:

          set_inds: Priors are created assuming properties will be provided in a tensor of
          shape n_neurons*n_total_prop_dims.  set_inds[i] gives the columns of the property tensor corresponding
          to the properties for property space i.

          n_divisions_per_dim: The number of divisons per dimension to use when generating hypercube functions.  There
          will be the same number of divisions for each dimension.

          n_div_per_hc_side_per_dim: The number of divisions per hypercube per dimension to use when generating
          hypercube functions.  This will be the same for all dimensions.

          init_mn: The initial mean for the conditional distribution for the p mode.  The mean will take on
          this value everywhere.

          init_std: The initial standard deviation for the conditional distribution for the p mode.  The standard
          deviation will take on this value everywhere.

          min_std: The minimum standard deviation the prior distributions can take on.

      Raises:
          ValueError: If min_std or init_std is less than 0
          ValueError: If init_std is less than min_std

      Returns:

          p_dists: p_dists[0] is the distribution for the p mode

          u_dists: u_dists[0] is None to signify there is no prior distribution over the u modes



.. py:function:: plot_single_2d_conditional_prior(priors: janelia_core.ml.torch_distributions.CondMatrixProductDistribution, mode: int, dim_sampling: Sequence[Sequence] = [[0, 1, 0.01], [0, 1, 0.01]], range: float = None, plot_mn: bool = True, t: numpy.ndarray = None)

   Plots a prior distribution of a neuron's loading given neuron's location in a 2-d space for a single mode.

   Args:
       priors: The conditional prior distribution over modes.

       mode: The index of the mode for the prior to plot.

       dim_sampling: Each entry of dim_sampling specifies how to sample a dimension in the
       domain of the function.  Each entry is of the form [start, stop, int] where start and
       and stop are the start and stop of the range of values to sample from and int
       is the interval values are sampled from.

       range: If provided, colors for values will saturate at +/- range.  If not provided, this
       is set based on the values to be plotted.

       plot_mn: If true, the mean will be plotted.  If false, the standard deviation is plotted.


.. py:function:: plot_2d_conditional_prior(priors: janelia_core.ml.torch_distributions.CondMatrixProductDistribution, dim_sampling: Sequence[Sequence] = [[0, 1, 0.01], [0, 1, 0.01]], mn_range: float = None, std_range: float = None)

   Plots a conditional prior distribution of a neuron's loading given neurons location in a 2-d space.

   The spatial means and standard deviations of each mode will be plotted in a 2 by M grid layout - each column
   is a mode with means plotted on top and standard deviations plotted on bottom.

   Args:
       priors: The conditional prior distribution over modes.

       dim_sampling: Each entry of dim_sampling specifies how to sample a dimension in the
       domain of the fuction.  Each entry is of the form [start, stop, int] where start and
       and stop are the start and stop of the range of values to sample from and int
       is the interval values are sampled from.

       mn_range: If provided, colors for mean values will saturate at +/- mn_range.  If not provided, this
       is set based on the means to be plotted.

       std_range: If provided, colors for standard deviation values will saturate at std_range.  If not provided,
       this is set based on the standard deviation values to be plotted.


.. py:function:: plot_single_2d_mode(mode: numpy.ndarray, neuron_p: numpy.ndarray, vl_range: float = None, dim_range: Sequence[Sequence] = [[0, 1], [0, 1]], plot_axes: matplotlib.axes = None, plot_color_bar: bool = True)

   Plots the mode loadings for a single mode for neurons positioned in 2-d space.

   Args:
       mode: The mode to plot as a 1-d array of length n_neurons.

       neuron_p: The position of each neuron.  Shape is n_neurons*2.

       vl_range: The min and max value to clip values at when assigning colors.  If not provided, will be
       assigned based on the values in modes.

       dim_range: The spatial range to generate plots for.

       plot_axes: The axes to plot into.  If none, one will be provided

       plot_color_bar: True if a color bar should be added to the plot.


.. py:function:: plot_single_2d_mode_posterior(post: janelia_core.ml.torch_distributions.CondMatrixProductDistribution, mode: int, neuron_p: numpy.ndarray, vl_range: float = None, dim_range: Sequence[Sequence] = [[0, 1], [0, 1]], plot_axes: matplotlib.axes = None, plot_mn: bool = True, t: numpy.ndarray = None, plot_color_bar: bool = True)

   Plots the mean or standard deviation of mode loadings for a single mode for neurons positioned in 2-d space.

   The distribution can be transformed before plotting.

   Args:
       post: The posterior distribution over modes.

       mode: The index of the mode to plot the posterior over.

       neuron_p: The position of each neuron.  Shape is n_neurons*2.

       vl_range: The min and max value to clip values at when assigning colors.  If not provided, will be
       assigned based on the values in modes.

       dim_range: The spatial range to generate plots for.

       plot_axes: Axes to plot into.  If None, one will be created.

       plot_mn: If true, the mean is plotted.  If false, the standard deviation is plotted.

       t: The transition matrix to apply to the distribution if it should be transformed before plotting.

       plot_color_bar: True if a color bar should be added to the plot.



.. py:function:: plot_2d_modes(modes: numpy.ndarray, neuron_p: numpy.ndarray, vl_range: float = None, dim_range: Sequence[Sequence] = [[0, 1], [0, 1]])

   Plots the mode loadings for neurons positioned in a 2-d space.

   Modes will be plotted in a row, followed by a colorbar.  All modes will be plotted on the same color scale.

   Args:
       modes: The modes to plot as a n_neurons by n_modes array. Each column is a mode.

       neuron_p: The position of each neuron.  Shape is n_neurons*2.

       vl_range: The min and max value to clip values at when assigning colors.  If not provided, will be
       assigned based on the values in modes.

       dim_range: The spatial range to generate plots for.


.. py:function:: learn_prior_transformation(d0, d1, dim_sampling: Sequence[Sequence] = [[0, 1, 0.01], [0, 1, 0.01]], d0_slice: slice = None, d1_slice: slice = None)

   Learns a transformation between two the means of two prior distributions.

   The transformation is learned based on sampling both conditional distributions.

   Args:
       d0: The distribution to transform *to*

       d1: The distribution to transform *from*

       dim_sampling: Each entry of dim_sampling specifies how to sample the conditional prior.  Each entry is
       of the form [start, stop, int] where start and and stop are the start and stop of the range of values to
       sample from and int is the interval values are sampled from.

       d0_slice: A slice indicating which modes of d0 to learn the transformation to.

       d1_slice: A slice indicating which modes of d1 to learn the transformation from.



