:py:mod:`janelia_core.ml.latent_regression.vi`
==============================================

.. py:module:: janelia_core.ml.latent_regression.vi

.. autoapi-nested-parse::

   Tools for fitting latent regression models with variational inference.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   janelia_core.ml.latent_regression.vi.PriorCollection
   janelia_core.ml.latent_regression.vi.SubjectVICollection
   janelia_core.ml.latent_regression.vi.MultiSubjectVIFitter



Functions
~~~~~~~~~

.. autoapisummary::

   janelia_core.ml.latent_regression.vi.format_output_list
   janelia_core.ml.latent_regression.vi.concatenate_check_points
   janelia_core.ml.latent_regression.vi.eval_fits
   janelia_core.ml.latent_regression.vi.predict
   janelia_core.ml.latent_regression.vi.predict_with_truth



.. py:function:: format_output_list(base_str: str, it_str: str, vls: Sequence[float], inds: Sequence[int])

   Produces a string to display a list of outputs.

   String will be of the format:

       base_str + ' ' + it_str+inds[0] + ': ' + str(vls[0]) + ', ' + it_str+inds[1] + ': ' + str(vls[1]) + ...

   Args:
       base_str: The base string that preceeds everything else on the string

       it_str: The string that should come befor each printed value

       vls: The values that should be printed

       inds: The indices to associate with each printed value

   Returns:
       f_str: The formatted string


.. py:function:: concatenate_check_points(check_points: Sequence[dict], params: Sequence[dict]) -> List

   Concatenates lists of checkpoints and computes total epochs to each checkpoint from multiple rounds of fitting.

   Args:
       check_points: check_points[i] is a sequence of check points produced by a call to
       MultiSubjectVIFitter.fit().  It is assumed that entries in check_points match the order they were produced
       in the actual fitting.

       params: params[i] is a dictionary for the call to MultiSubjectVIFitter.fit() that produced the checkpoints
       in check_points[i].  Is should have the fields:
           cp_epochs: For the epochs that the checkpoints were created at
           n_epochs: For the total number of epochs that fitting was run for

   Returns:

       conc_check_points: The concatenated list of check points

       cp_epochs: The total accumulated epochs that were run to get to each check point.

       orig_cp_fits: orig_cp_fits[0,i] is the index of the fit for the i^th concatenated checkpoint and
                     orig_cp_fits[1,i] is the index of this check point in the logs for that fit.


.. py:class:: PriorCollection(p_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None], u_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None], psi_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None], scale_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None] = None, offset_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None] = None, direct_mapping_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None] = None)

   Holds prior distributions when fitting models with variational inference.

   This object offers convenience functions for getting all the parameters for the priors as well as moving the
   distributions to different devices.

   Creates a new PriorCollection object.

   When specifying a prior distribution over a parameter, the user can specify either a CondVAEDistribution object,
   which will be used if posterior distributions are fit over the parameter.  Alternatively, if point estimates
   will be fit over the parameter, the user can provide the value None.  E.g., if a distribution will be fit over
   the first input group modes but not the second p_dists would be set to [d, None], where d is a
   CondVAEDistribution object.

   Args:
       p_dists: Prior distributions over p modes.

       u_dists: Prior distributions over u modes.

       psi_dists: Prior distributions over variance parameters.

       scale_dists: Prior distributions over scale parameters.  If scale parameters are not used in subject
       models, set this to None.

       offset_dists: Prior distributions over offset parameters.  If offset parameters are not use in subject
       models, set this to None.

       direct_mapping_dists: Prior distributions over direct mappings.  direct_mapping_dists[i] should be the
       prior over the direct mappings in direct_pairs[i] in subject  models.  If direct mappings are not used,
       set this to None.


   .. py:method:: get_used_devices(self)

      Returns all devices that priors are on.


   .. py:method:: r_params(self) -> List[torch.nn.parameter.Parameter]

      Gets parameters of all modules for which gradients can be estimated with the reparameterization trick.


   .. py:method:: s_params(self) -> List[torch.nn.parameter.Parameter]

      Gets parameters of all modules for which gradients can be estimated with the score method.


   .. py:method:: to(self, device: Union[torch.device, int])

      Moves all distributions in the collection to a specified device.



.. py:class:: SubjectVICollection(s_mdl: janelia_core.ml.latent_regression.subject_models.LatentRegModel, p_dists: Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], u_dists: Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], psi_dists: Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], data: janelia_core.ml.datasets.TimeSeriesBatch, input_grps: Sequence[int], output_grps: Sequence[int], props: Union[Sequence[torch.Tensor], None], p_props: Sequence[int], u_props: Sequence[int], psi_props: Sequence[int], scale_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None] = None, offset_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None] = None, direct_mappings_dists: Union[Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], None] = None, scale_props: Union[Sequence[int], None] = None, offset_props: Union[Sequence[int], None] = None, direct_mapping_props: Union[Sequence[int], None] = None, min_var: Union[Sequence[float], None] = None)

   Holds data, likelihood models and posteriors for fitting data to a single subject with variational inference.

   This object offers convenience functions to get all the trainable parameters for a subject model and its posteriors
   as well as moving everything needed for fitting for that subject to different devices.

   Creates a new SubjectVICollection object.

   When specify the distribution over a parameter of the model, the user can specify either (1) a
   janelia_core.ml.torch_distributions.CondVAEDistribution object or (2) None.  Specifying a distribution
   means that distribution will be used as the posterior distribution over the parameter for fitting.  Specifying
   None, means that a distribution won't be fit for that parameter.  Instead, a point estimate will be fit by
   directly optimizing the value of the parameter in the subject model.

   Args:
       s_mdl: The likelihood model for the subject.

       p_dists: The posterior distributions for the p modes.

       u_dists: The posterior distributions for the u modes.

       psi_dists: The posterior distributions for psi parameters.

       data: Data for the subject.

       input_grps: input_grps[g] is the index into data.data for the g^th input group

       output_grps: output_grps[h] is the index into data.data for the h^th output group

       props: props[i] is a tensor of properties.  If there are no properties, this should be None.

       p_props: p_props[g] is the index into props for the properties for the modes for the g^th input group.  If
       there are no distributions over the modes for this group, p_props[g] should be None. p_props[g] should also
       be None if there are distributions for these modes but they are not conditioned on anything.

       u_props:  p_props[h] is the index into props for the properties for the modes for the h^th output group,
       same format as u_props.

       psi_props:  psi_props[h] is the index into props for the properties for the variances for the h^th output
       group, same format as u_props.

       scale_dists: The posterior distributions for scale parameters. If scales are not used in the model,
       set to None.

       offset_dists: The posterior distributions for offset parameters, same form as scale_dists.

       direct_mappings_dists: Distributions over direct mappings.  If there are no direct mappings in the model,
       this should be None.  If there are direct mappings, direct_mapping_dists[i] is the distribution over
       s_mdl.direct_mappings[i].

       scale_props:  scale_props[h] is the index into props for the properties for the scales for the h^th output
       group, same format as u_props.

       offset_props:  offset_props[h] is the index into props for the properties for the offsets for the h^th output
       group, same format as u_props.

       direct_mapping_props: direct_mapping_props[i] is the index into props for the properties for the i^th
       direct mapping, same format as u_props

       min_var: min_var[h] is the minimum variance for the additive noise variables for output group h. If
       set to None, min_var for all output groups will be set to .01.


   .. py:method:: trainable_parameters(self) -> List[torch.nn.parameter.Parameter]

      Returns all trainable parameters for the collection.

      Returns:
          params: The list of parameters.


   .. py:method:: to(self, device: Union[torch.device, int], distribute_data: bool = False)

      Moves all relevant attributes of the collection to the specified device.

      Note that by default fitting data will not be moved to the device.

      Args:
          device: The device to move attributes to.

          distribute_data: True if fitting data should be moved to the device as well



.. py:class:: MultiSubjectVIFitter(s_collections: Sequence[SubjectVICollection], prior_collection: PriorCollection, input_modules: Union[torch.nn.ModuleList, None] = None, penalizers: Union[Sequence[janelia_core.ml.torch_parameter_penalizers.ParameterPenalizer], None] = None)

   Object for fitting a collection of latent regression models with variational inference.



   Creates a new MultiSubjectVIFitter object.

   Args:
       s_collections: A set of SubjectVICollections to use when fitting data.

       prior_collection: The prior collection to use when fitting data.

       input_modules: Contains modules to apply to inputs before they are passed to subject models.  If None,
       no input modules will be used.  If a list, the g^th module is the module to apply to the input for
       input group g.  If no module should be applied to a group, the entry for that group should be None.

       penalizers: Parameter penalizers that will be used when fitting.

   .. py:method:: create_check_point(self, inc_penalizers: bool = False) -> dict

      Returns copies of subject vi collections, priors as well as (optionally) penalizer parameters.

      Args:
          inc_penalizers: True if copies of penalizer parameters should be returned.

      Returns:
          cp_dict: A dictionary with the following keys:

              s_collections: Copies of the subject collections, with data and properties removed

              prior_collection: Copy of the prior collection

              input_modules: Copy of input modules.

              parameter_penalizer_dicts: If penalizer parameters are requested, then parameter_penalizer_dicts[i] is
              the dictionary witch check point parameters for the i^th parameter penalizer, where the ordering of
              penalizers is the same as when parameter penalizers were provided at the time of the creation of the
              Fitter object.


   .. py:method:: distribute(self, devices: Sequence[Union[torch.device, int]], s_inds: Sequence[int] = None, distribute_data: bool = False)

      Distributes priors, subject collections and penalizers across devices.

      Args:
          devices: Devices that priors and subject collections should be distributed across.

          s_inds: Indices into self.s_collections for subject models which should be distributed across devices.
          If none, all subject models will be distributed.

          distribute_data: True if all training data should be distributed to devices.  If there is enough
          device memory, this can speed up fitting.  If not, set this to false, and batches of data will
          be sent to the device for each training iteration.



   .. py:method:: trainable_parameters(self, s_inds: Sequence[int] = None, get_prior_params: bool = True) -> [list, list]

      Gets all trainable parameters for fitting.

      This function returns separate parameters for penalizer parameters and all other parameters, to faciltate
      applying different learning rates to the penalizer parameters.

      Args:
          s_inds: Specifies the indices of subjects that will be fit.  Subject indices correspond to their
          original order in s_collections when the fitter was created. If None, all subjects used.

          get_prior_params: True if parameters of priors should be included in the set of returned parameters

      Returns:
           base_params: Parameters of priors, posteriors, input modules and subject models

           penalizer_params: Parameters of any penalizers


   .. py:method:: get_penalizer_params(self, keys: Union[str, Sequence[str]] = None) -> list

      Returns a list of penalizer parameters, optionally filtering by key.

      Args:
          keys: If provided, either a string of a single key that returned parameters should match or a
          sequence of keys parameters can match.  Any parameters not matching the requested key(s), will
          not be returned.  If keys is None, all penalizer parameters will be returned.

      Returns:
          params: A list of the requested parameters



   .. py:method:: generate_batch_smp_inds(self, n_batches: int, s_inds: Sequence[int] = None) -> List[List[numpy.ndarray]]

      Generates indices of random mini-batches of samples for each subject.

      Args:
          n_batches: The number of batches to break the data up for each subject into.

          s_inds: Specifies the indices of subjects that will be fit.  Subject indices correspond to their
          original order in s_collections when the fitter was created. If None, s_inds = range(n_subjects).

      Returns:
          batch_smp_inds: batch_smp_inds[i][j] is the sample indices for the j^th batch for subject s_inds[i]



   .. py:method:: get_used_devices(self)

      Lists any device currently used for fitting.

      Returns:
          devices: The list of devices parameters are on in.


   .. py:method:: fit(self, n_epochs: int = 10, n_batches: int = 10, learning_rates=0.01, adam_params: dict = {}, s_inds: Sequence[int] = None, fix_priors: bool = False, enforce_priors: bool = True, update_int: int = 1, print_opts: dict = None, cp_epochs: Sequence[int] = None, cp_penalizers: bool = True) -> [dict, Union[List, None]]

      Args:

          n_epochs: The number of epochs to run fitting for.

          n_batches: The number of batches to break the training data up into per epoch.  When multiple subjects have
          different numbers of total training samples, the batch size for each subject will be selected so we go
          through the entire training set for each subject after processing n_batches each epoch.

          learning_rates: If a single number, this is the learning rate to use for all epochs and parameters.
          Alternatively, this can be a list of tuples.  Each tuple is of the form
          (epoch, base_lr, penalizer_lr_opts), where epoch is the epoch the learning rates come into effect on,
          base_lr is the learning rate for all parameters other than the penalizer parameters and penalizer_lr_opts
          is a dictionary with keys specifying penalizer parameter keys and values giving the learning rate
          for those parameters. Multiple tuples can be provided to give a schedule of learning rates.  Here is an
          example learning_rates: [(0, .001, {'fast': 1, 'slow', .1}), (100, .0001, {'fast': .1, 'slow', .01}] that
          starts with a base learning rates of .001, a learning rate of 1 for parameters of the penalizers that
          should be assigned a fast learning rate and a learning rate of .1 for parameter of the penalizers that
          should be assigned a slow learning rate.  At epoch 100, the learning rates are divided by 10 in this
          example.

          adam_params: Dictionary of parameters to pass to the call when creating the Adam Optimizer object.
          Note that if learning rate is specified here *it will be ignored.* (Use the learning_rates option instead).
          The options specified here will be applied to all parameters at all iterations.

          s_inds: Specifies the indices of subjects to fit to.  Subject indices correspond to their
          original order in s_collections when the fitter was created. If None, all subjects used.

          fix_priors: True if priors should be fixed and not changed during fitting

          enforce_priors: True if we should calculate kl divergences between priors and posteriors and include
          this in the objective.

          update_int: Fitting status will be printed to screen every update_int number of epochs

          print_opts: Options controlling what is printed to screen.  See _print_fitting_status()

          cp_epochs: A sequence of epochs after which a check point of the models (as well as optionally the
          penalizers will be made).  If no check points should be made, set this to None.

          cp_penalizers: True if penalizers should be included in the check points.

      Return:
          log: A dictionary with the following entries:

              'elapsed_time': A numpy array of the elapsed time for completion of each epoch

              'mdl_nll': mdl_nll[e, i] is the negative log likelihood for the subject model s_inds[i] at the start
              of epoch e (that is when the objective has been calculated but before parameters have been updated)

              'sub_p_kl': sub_p_kl[e,i] is the kl divergence between the posterior and conditional prior for the p
              modes for subject i at the start of epoch e.

              'sub_u_kl': sub_u_kl[e,i] is the kl divergence between the posterior and conditional prior the u modes
              for subject i at the start of epoch e.

              'p_prior_penalties': p_prior_penalties[e, :] is the penalty calculated for each group of p priors at the
              start of epoch e.

              'u_prior_penalties': u_prior_penalties[e, :] is the penalty calculated for each group of u priors at the
              start of epoch e.

              parameter_penalties: parameter_penalties[e, :] is the penalty calculated for each parameter penalizer,
              with the order of entries in each row corresponding to the order penalizers were provided when creating
              the Fitter object.

              obj: obj[e] contains the objective value at the start of epoch e.  This is the negative evidence lower
              bound + weight penalties.

          check_points: check_points[i] are model parameters for the i^th requested checkpoint. If no check points
          were requested this will be None.

      Raises:
          RuntimeError: If distribute() has not been called before fitting.
          ValueError: If sample_posteriors is False but enforce_priors is True.
          ValueError: If weight_penalty_type is not 'l1' or 'l2'.



   .. py:method:: plot_log(cls, log: dict, show_obj: bool = True, show_mdl_nll: bool = True, show_p_kl: bool = True, show_u_kl: bool = True, show_psi_kl: bool = True, show_scales_kl: bool = True, show_offsets_kl: bool = True, show_direct_mappings_kl: bool = True, show_penalties: bool = True)
      :classmethod:

      Produces a figure of the values in a log produced by fit().

      Args:
          log: The log to plot.

          show_obj: True if the objective should be plotted through time

          show_mdl_nll: True if model negative log likelihood should be plotted through time

          show_p_kl: True if the kl divergences between prior and posterior distributions for p modes should be
                     plotted through time.

          show_u_kl: True if the kl divergences between prior and posterior distributions for u modes should be
                     plotted through time.

          show_psi_kl: True if the kl divergences between prior and posterior distributions for psi parameters should
                       be plotted through time.

          show_scales_kl: True if the kl divergences between prior and posterior distributions for scale parameters
                          should be plotted through time.

          show_offsets_kl: True if the kl divergences between prior and posterior distributions for offset parameters
                           should be plotted through time.

          show_direct_mappings_kl: True if the kl divergences between prior and posterior distributions for
                                   direct mapping parameters should be plotted through time.



   .. py:method:: to(self, device: torch.device, distribute_data: bool = False)

      Move everything in the fitter to a specified device.

      This is most useful when wanting to clean up after fitting and you need to move models to CPU.

      Args:
          device: The device to move everything to (e.g., torch.device('cpu'))

          distribute_data: True if data should be moved as well.



   .. py:method:: _calc_kl_and_backward(dists0: Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], dists1: Sequence[Union[janelia_core.ml.torch_distributions.CondVAEDistribution, None]], props: Sequence[torch.Tensor], prop_inds: Union[Union[Sequence[int], None], None], smps: Sequence[Union[torch.Tensor, None]])
      :staticmethod:

      Helper function for computing KL divergence between posterior and prior distributions and calling backwards.

      This functions handles the case when distributions and/or properties are not provided.

      Args:

          dists0: Posterior distributions.  If a None value is provided in the list, this serves as a placeholder
          indicating no calculations should be done.

          dist1: Prior distributions.

          props: List of properties

          prop_inds: props_inds[i] is the index in props that dists0[i] and dists[1] i should be conditioned on.

          smps: smps[i] is a set of samples to use for computing kl divergence.  Can be None if the kl calculation
          for a distribution type is known analytically.

      Returns:

          kl: The computed kl divergence



   .. py:method:: _move_dists(dists: List, device: torch.device)
      :staticmethod:

      Moves distributions to a device, avoiding calling .to() on None entries in a list.


   .. py:method:: _print_fitting_status(e_i: int, elapsed_time: float, batch_obj: float, cur_learning_rates, s_inds: Sequence[int], batch_nll: Sequence[float], batch_sub_p_kl: Sequence[float], batch_sub_u_kl: Sequence[float], batch_sub_psi_kl: Sequence[float], batch_sub_scales_kl: Sequence[float], batch_sub_offsets_kl: Sequence[float], batch_sub_direct_mappings_kl: Sequence[float], batch_penalties: Sequence[float], penalizers: Sequence[janelia_core.ml.torch_parameter_penalizers.ParameterPenalizer], devices: List[torch.device], print_opts: dict)
      :staticmethod:

      Helper function for printing updates on fitting process.

      Args:
          e_i: The epoch index

          elapsed_time: Elapsed fitting time

          batch_obj: The objective value for the a batch of data (This will typically be the last batch of data
          for the epoch)

          cur_learning_rates: The current learning rates

          s_inds: The indices of the subjects that are being fit

          batch_nll: The negative log-likelihoods for the batch for each fit subject model
          (should correspond to s_inds)

          batch_sub_p_kl: The kl divergences for the p-mode distributions for each subject for the batch.

          batch_sub_u_kl: The kl divergences for the u-mode distributions for each subject for the batch.

          batch_sub_psi_kl: The kl divergences for the psi parameter distributions for each subject for the batch.

          batch_sub_scales_kl: The kl divergences for the scale parameter distributions for each subject for the
          batch.

          batch_sub_offsets_kl: The kl divergences for the offset parameter distributions for each subject for the
          batch.

          batch_sub_direct_mappings_kl: The kl divergences for the direct mapping parameter distributions for each
          subject for the batch.

          batch_penalties: The penalties each penalizer produced for the batch.

          penalizers: The penalizers used in fitting.

          devices: List of devices to print memory stats for.

          print_opts: A dictionary with fields with boolean values indicating which information should be shown.
          The fields are: mdl_nll, sub_kls, penalties, memory_usage.  Any fields that are not provided will be
          assumed to be false.


   .. py:method:: _sample_posteriors(dists: List[janelia_core.ml.torch_distributions.CondVAEDistribution], all_props: List[torch.Tensor], prop_inds: List[int], squeeze_std_smp: bool = False)
      :staticmethod:

      Samples posteriors, returning samples in compact and standard form.

      Args:
          dists: List of distributions to sample.  Entries can be None (allowing for placeholders)

          all_props: List of property tensors.

          prop_inds: prop_inds[i] contains is the index in all_props for dists[i].  If prop_inds[i] is None,
          then sampling is done without conditioning on properties.

      Returns:

          smps: smps[i] is the compact samples for dists[i].  If dists[i] was none, smps[i] will ne none.

          std_smps: std_smps[i] is smps[i] in standard form.  If dists[i] was none, std_smps[i] will be none.

          squeeze_std_sample: True if standard sample should be squeezed before returning.



.. py:function:: eval_fits(s_collections: Sequence[SubjectVICollection], input_modules: Sequence[torch.nn.ModuleList], data: janelia_core.ml.datasets.TimeSeriesBatch, batch_size: int = 100, metric: Callable = None, return_preds: bool = True, sample: bool = False) -> List

   Measures model fits on a given set of data.

   This function generates predictions for each model using the posterior means of modes. It then evaluates these
   predictions using negative log-likelihood by default but the user can specify other metrics for measuring
   prediction quality.

   Args:
       s_collections: A sequence of VI collections we want to evaluate.

       input_modules: A sequence of input modules, corresponding to the input modules that should be used
       to preprocess the input for each collection in s_collections.

       data: The data to use for evaluation

       batch_size: The number of samples to send to GPU at one time for evaluation; can be useful if working with
       low-memory GPUs.

       metric: A function which computes fit quality given the output of predict_with_truth

       return_preds: True if predictions should be returned

       sample: True if when forming model parameters, instead of using means of posteriors, samples from the
       posteriors should be used instead.

   Returns:
       metrics: metrics[i] is the fit quality for s_collections[i].  Note that if no metric is supplied, this will
       just be a list of negative log-likelihood values, but custom metric functions can return arbitrary objects.

       preds_with_truth: preds_with_truth[i] are the predictions for s_collections[i] produced with the function
       predict_with_turth. This will only be returned in return_preds is true.


.. py:function:: predict(s_collection: SubjectVICollection, input_modules: torch.nn.ModuleList, data: janelia_core.ml.datasets.TimeSeriesBatch, batch_size: int = 100, sample: bool = False) -> List[numpy.ndarray]

   Predicts output given input from a model with posterior distributions over parameters.

   If posterior distributions for a parameter are present, predictions are based on the posterior mean by
   default (but see sample input for using samples instead).  If posterior distributions for a parameter are not
   present, the parameter value in the subject model will be used.

   Note: All predictions will be returned on host (cpu) memory as numpy arrays.

   Args:
       s_collection: The collection for the subject.   Any data in the collection will be ignored.

       input_modules: Input modules for preprocessing data.

       data: The data to predict with.

       batch_size: The number of samples we predict on at a time.  This is helpful if using a GPU
       with limited memory.

       sample: If true, instead of using posterior means for model parameters when forming predictions,
       the posteriors will be sampled and these samples will be used for model parameters.

   Returns:
       pred_mn: The predicted means given the input.


.. py:function:: predict_with_truth(s_collection: SubjectVICollection, input_modules: torch.nn.ModuleList, data: janelia_core.ml.datasets.TimeSeriesBatch, batch_size: int = 100, time_grp: int = None, sample: bool = False)

   Predicts output for a model, using posterior over modes, and including true data in output for reference.

   This is a wrapper function around predict for convenience.

   Note: All predictions will be returned on host (cpu) memory as numpy arrays.

   Args:
       s_collection: The collection for the subject.   Any data in the collection will be ignored.

       input_modules: Input modules for preprocessing input.

       data: The data to predict with.

       batch_size: The number of samples we predict on at a time.  This is helpful if using a GPU
       with limited memory.

       time_grp: The index of the group in data with time stamps.  If None, no time stamps will be returned.

       sample: True if instead of using posterior means for model parameters when making predictions, samples from
       each distribution should be used instead.

   Returns:

       predictions: A dictionary with the following keys:
           pred: predictions. pred[h] is the prediction for the h^th output group of the model
           truth: Corresponding true values for those in pred
           time: It time_grp is not None, the time indices for each point in pred and truth. Otherwise, time will be
           None.



