:py:mod:`janelia_core.ml.torch_parameter_penalizers`
====================================================

.. py:module:: janelia_core.ml.torch_parameter_penalizers

.. autoapi-nested-parse::

   Holds modules for penalizing torch parameters.

   See the base class ParameterPenalizer for more information.



Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   janelia_core.ml.torch_parameter_penalizers.ParameterPenalizer
   janelia_core.ml.torch_parameter_penalizers.ClusterPenalizer
   janelia_core.ml.torch_parameter_penalizers.TargetLengthPenalizer
   janelia_core.ml.torch_parameter_penalizers.UnsignedClusterPenalizer
   janelia_core.ml.torch_parameter_penalizers.ScalarPenalizer
   janelia_core.ml.torch_parameter_penalizers.UnsignedScalarPenalizer




.. py:class:: ParameterPenalizer(params: Sequence[torch.nn.Parameter])

   Bases: :py:obj:`abc.ABC`, :py:obj:`torch.nn.Module`

   An abstract class for parameter penalizers.

   The main idea behind a penalizer object is that when fitting models instead of simply applying a penalty function,
   we might want to penalize parameters with respect to a function which also has its own learnable parameters. For
   example, lets say there are a group of parameters and we want to penalize things so that the l_2 distance between
   all the parameters in that group is small. Instead of calculating the l_2 distance between all pairs of parameters
   and penalizing the sum of those distances, it might be simpler to have a penalizer with a center parameter which is
   learned and penalizing the distance of each parameter in the group to the center.

   Note one point of confusion is there are now two sets of parameters - one set is the set of parameters that we
   want to penalize and the other set is the set of internal, learnable parameters of the penalizer itself.  In
   the example above, the center would be an internal, learnable parameter of the penalizer.


   Creates an instance of a penalizer.

   Args:
       params: The parameters to penalize over.

   .. py:method:: check_point() -> dict
      :abstractmethod:

      Returns a dictionary of parameters and values for the penalizer that should be saved in a check point.




   .. py:method:: clone(clean: bool = True)
      :abstractmethod:

      Returns a copy of self.

      Args:
          clean: If true, attribute values that we might not want to transfer to a new object (such as record of last
          penalty value) will not be copied.

      Returns:
          obj: The new object


   .. py:method:: copy_state_from(other)
      :abstractmethod:

      Copies the state of one penalizer to this penalizer.

      State should be the internal parameters of the penalizers as well as other internal varialbes it may keep but
      it should not include the parameters the penalizer actually penalizes.

      Args:
          other: The other penalizer to copy state from


   .. py:method:: get_marked_params(key: str) -> List[torch.nn.Parameter]
      :abstractmethod:

      Returns all learnable parameters marked with the key string.

      Penalizers must associate each of their internal, learnable parameters with a unique key
      (e.g., fast_learning_rate_params).  Each parameter should be associated with only one key
      (though multiple parameters can use the same key).  This function will return a list of parameters
      associated with the requested key.  If no parameters match the key an empty list should be returned.


   .. py:method:: list_param_keys() -> List[str]
      :abstractmethod:

      Returns a list of keys associated with internal, learnable parameters.

      Returns:
          keys: The list of keys.


   .. py:method:: penalize_and_backwards(call_backwards: bool) -> float
      :abstractmethod:

      Calculates a penalty over parameters and calls backwards on the penalty.

      The reason for having the penalizer call backwards is that there may be complicated situations, such as
      when parameters are spread over multiple GPUs, that we need to call backwards multiple times as we
      move things between GPUs when calculating the penalty.

      Args:
          d: The distribution to penalize

      Returns:
          penalty: The scalar penalty.  Note this is a float and not a tensor, as we assume backwards has
          been called in this function.


   .. py:method:: __str__() -> str
      :abstractmethod:

      Returns a string of the current state of the penalizer.



.. py:class:: ClusterPenalizer(params: Sequence[torch.nn.Parameter], w: float, init_ctr: torch.Tensor, description: str = '', learnable_parameters: bool = True)

   Bases: :py:obj:`ParameterPenalizer`

   This penalizer encourages clustering of parameters in tensors.

   In particular, given a set of paremters p_0, ..., p_N of arbitrary shape, the penalty computed by this object is:

       w\sum_{i=1}^N ||p_i - c||_2^2 where c is a tensor the same shape as any of the p_i parameters representing
       the center of a cluster and w is a penalty weight.

   There is only one parameter for this penalizer, which is tagged with 'fast', to indicate that in models trained
   with slow and fast learning rates, we would expect the center to be updated with the fast learning rate.

   Creates an instance of a ClusterPenalizer.

   Args:
       params: The parameters to penalize

       w: The weight to apply the penalty

       init_ctr: The initial value of c

       description: A string that will be used to identify the penalizer in the string returned
       by __str__()

       learnable_parameters: True if c should be learnable; false if it should be fixed

   .. py:method:: copy_state_from(other)

      Copies the state of another penalizer to this penalizer.

      Args:
          other: The other penalizer to copy state form.


   .. py:method:: clone(clean: bool = True)

      Returns a copy of self.

      Args:
          clean: If true, attribute values that we might not want to transfer to a new object (such as record of last
          penalty value) will not be copied.

      Returns:
          obj: The new object


   .. py:method:: check_point() -> dict

      Returns a check point dictionary for the penalizer.

      Returns:
          d: A dictionary with the following keys:

              c: The center of the parameter

              last_p: The value of the last penalty that was computed


   .. py:method:: get_marked_params(key: str) -> List[torch.nn.Parameter]

      Returns marked parameters.

      The only parameter of the penalizer is the centers tensor, c, which is marked with the tag 'fast'

      Returns:
          params: A list.  If the key was fast this will hold the 'c' parameter.  If not, this list will be empty.


   .. py:method:: list_param_keys() -> List[str]

      Returns the list of keys associated with internal, learnabke parameters.


   .. py:method:: penalize_and_backwards(call_backwards: bool = True) -> torch.Tensor

      Computes the penalty over the parameters and then calls backwards.

      Args:
          call_backwards: True if backwards should be called.

      Returns:
          penalty: The calculated penalty


   .. py:method:: __str__()

      Returns a string with the state of the penalizer.



.. py:class:: TargetLengthPenalizer(params: Sequence[torch.nn.Parameter], w: float, tgt_l: float, description: str = '')

   Bases: :py:obj:`ParameterPenalizer`

   Penalizes the l-2 norm of a parameter as it deviates from a target length.

   The l-2 norm is calculated by summing the square of all elements in a parameter, no matter what it's shape is,
   and then taking the square root.

   The penalty for a parameter is calculated as: w*(l - tgt_l)**2, where w is a penalty weight, l is the l_2 norm
   of the parameter and tgt_l is the target length we would like the parameter to have.

   This object can hold multiple parameters and will return the sum of penalizing all of them.

   Creates a new TargetLengthPenalizer object.

   Args:
       params: the parameters to penalize.  Each parameter will be treated independently.

       w: the weight to apply to the penalty

       tgt_l: The target length for each parameter

       description: A short description identifying the penalizer.

   .. py:method:: copy_state_from(other)

      Copies the state of another penalizer to this penalizer.

      Args:
          other: The other penalizer to copy state form.


   .. py:method:: clone(clean: bool = True)

      Returns a copy of self.

      Args:
          clean: If true, attribute values that we might not want to transfer to a new object (such as record of last
          penalty value) will not be copied.

      Returns:
          obj: The new object


   .. py:method:: check_point() -> dict

      Returns a check point dictionary for the penalizer.

      Returns:
          d: A dictionary with the following keys:

              last_p: The value of the last penalty that was computed


   .. py:method:: get_marked_params(key: str) -> List[torch.nn.Parameter]

      Returns marked parameters.

      There are no learnable parameters for this object, so this function always returns an empty list.

      Returns:
          params: A list. This will always be empty.


   .. py:method:: list_param_keys() -> List[str]

      Returns the list of keys associated with internal, learnable parameters.

      Because there are no learnable parameters for this penalizer, an empty list will be returned.


   .. py:method:: penalize_and_backwards(call_backwards: bool = True) -> torch.Tensor

      Computes the penalty over the parameters and then calls backwards.

      Args:
          call_backwards: True if backwards should be called.

      Returns:
          penalty: The calculated penalty


   .. py:method:: __str__()

      Returns a string with the state of the penalizer.



.. py:class:: UnsignedClusterPenalizer(params: Sequence[torch.nn.Parameter], w: float, init_ctr: torch.Tensor, description: str = None, learnable_parameters: bool = True)

   Bases: :py:obj:`ClusterPenalizer`

   This is the same as the ClusterPenalizer but the penalty is computed after taking absolute values of parameters.

   In particular, given a set of paremters p_0, ..., p_N of arbitrary shape, the penalty computed by this object is:

       w\sum_{i=1}^N ||abs(p_i) - abs(c)||_2^2 where c is a tensor the same shape as any of the p_i parameters
       representing the center of a cluster and w is a penalty weight.

   Creates a new UnsignedClusterPenalizer instance.

   See __init__ of parent for more information.

   .. py:method:: penalize_and_backwards(call_backwards: bool = True)

      Computes the penalty over the parameters and then calls backwards.

      Args:
          call_backwards: True if backwards should be called.

      Returns:
          penalty: The calculated penalty



.. py:class:: ScalarPenalizer(params: Sequence[torch.nn.Parameter], w: float, init_ctr: float, description: str = None, learnable_parameters: bool = True)

   Bases: :py:obj:`ParameterPenalizer`

   Applies an element-wise penalty to parameters, which is the squared distance of each element from a center.

       In particular, given a set of paremters p_0, ..., p_N of arbitrary shape, the penalty computed by this object is:

           w\sum_{i=1}^N ||p_i - c||_2^2 where c the scalar center.

   There is only one parameter for this penalizer, which is tagged with 'fast', to indicate that in models trained
   with slow and fast learning rates, we would expect the center to be updated with the fast learning rate.


   Creates an instance of a ClusterPenalizer.

   Args:
       params: The parameters to penalize

       w: The weight to apply to the penalty

       init_ctr: The initial value of c

       description: A string that will be used to identify the penalizer in the string returned
       by __str__()

       learnable_parameters: True if c should be learnable; false if it should be fixed

   .. py:method:: copy_state_from(other)

      Copies the state of another penalizer to this penalizer.

      Args:
          other: The other penalizer to copy state form.


   .. py:method:: clone(clean: bool = True)

      Returns a copy of self.

      Args:
          clean: If true, attribute values that we might not want to transfer to a new object (such as record of last
          penalty value) will not be copied.

      Returns:
          obj: The new object


   .. py:method:: check_point() -> dict

      Returns a check point dictionary for the penalizer.

      Returns:
          d: A dictionary with the following keys:

              c: The center of the parameter

              last_p: The value of the last penalty that was computed


   .. py:method:: get_marked_params(key: str) -> List[torch.nn.Parameter]

      Returns marked parameters.

      The only parameter of the penalizer is the centers tensor, c, which is marked with the tag 'fast'

      Returns:
          params: A list.  If the key was fast this will hold the 'c' parameter.  If not, this list will be empty.


   .. py:method:: list_param_keys() -> List[str]

      Returns the list of keys associated with internal, learnable parameters.


   .. py:method:: penalize_and_backwards(call_backwards: bool = True) -> torch.Tensor

      Computes the penalty over the parameters and then calls backwards.

      Args:
          call_backwards: True if backwards should be called.

      Returns:
          penalty: The calculated penalty


   .. py:method:: __str__()

      Returns a string with the state of the penalizer.



.. py:class:: UnsignedScalarPenalizer(params: Sequence[torch.nn.Parameter], w: float, init_ctr: float, description: str = None, learnable_parameters: bool = True)

   Bases: :py:obj:`ScalarPenalizer`

   Penalizes the elements of parameters, with the squared distance of each element from a center.

       In particular, given a set of paremters p_0, ..., p_N of arbitrary shape, the penalty computed by this object is:

       w\sum_{i=1}^N ||abs(p_i) - abs(c)||_2^2 where c the scalar center.

   There is only one parameter for this penalizer, which is tagged with 'fast', to indicate that in models trained
   with slow and fast learning rates, we would expect the center to be updated with the fast learning rate.

   Creates a new instance of an UnsignedScalarPenalizer object.

   See __init__() of paraent for more information.

   .. py:method:: penalize_and_backwards(call_backwards: bool = True) -> torch.Tensor

      Computes the penalty over the parameters and then calls backwards.

      Args:
          call_backwards: True if backwards should be called.

      Returns:
          penalty: The calculated penalty



