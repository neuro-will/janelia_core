{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing of Pytorch with cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from janelia_core.ml.utils import format_and_check_learning_rates\n",
    "from janelia_core.ml.latent_regression import IdentityMap\n",
    "from janelia_core.ml.extra_torch_modules import Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentRegModel(torch.nn.Module):\n",
    "    \"\"\" A latent variable regression model.\n",
    "\n",
    "    In this model, we have G groups of input variables, x_g \\in R^{d_in^g} for g = 1, ..., G and\n",
    "    H groups of output variables, y_h \\in R^{d_out^h} for h = 1, ..., H\n",
    "\n",
    "    We form G groups of \"projected\" latent variables as proj_g = p_g^T x_g, for proj_g \\in R^{d_proj^g},\n",
    "    Note that p_g need not be an orthonormal projection.\n",
    "\n",
    "    There are also H sets of \"transformed\" latent variables, tran_1, ..., tran_H, with tran_h \\in R^{d_trans^h}, where\n",
    "    d_trans^h is the dimensionality of the transformed latent variables for group h.\n",
    "\n",
    "    Each model is equipped with a mapping, m, from [proj_1, ..., proj_G] to [tran_1, ..., tran_G].  The mapping m may\n",
    "    have it's own parameters.  The function m.forward() should accept a list, [proj_1, ..., proj_G], as input where\n",
    "    proj_g is a tensor of shape n_smps*d_proj^g and should output a list, [tran_1, ..., tran_G], where trah_h is a\n",
    "    tensor of shape n_smps*d_trans^h.\n",
    "\n",
    "    The transformed latents are mapped to a high-dimensional vector z_h = u_h tran_h, where z_h \\in R^{d_out^h}.\n",
    "\n",
    "    In addition, the user can specify pairs (g, h) when d_in^g = d_out^h, where there is a direct mapping for the\n",
    "    from x_g to a vector v_h, v_h = c_{h,g} x_g, where c_{h,g} is a diagonal matrix.  This is most useful when x_g and\n",
    "    y_g are the same set of variables (e.g, neurons) at times t-1 and t, and in addition to low-rank interactions,\n",
    "    we want to include interactions between each variable and itself.\n",
    "\n",
    "    Variables o_h = z_h + v_h are then formed (if there is an h for which v_h is not computed, then o_h = z_h.)\n",
    "\n",
    "    A (possibly) non-linear function s_h is applied to form mn_h = s_h(o_h) \\in R^{d_out^h}. s_h can\n",
    "    again have it's own parameters. s_h can general function mapping from R^{d_out^h} to R^{d_out^h},\n",
    "    but in many cases, it may be a composite function which just applies the same function to o_h, element-wise.\n",
    "\n",
    "    Finally, y_h = mn_h + n_h, where n_h ~ N(0, psi_h) where psi_h is a diagonal covariance matrix.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in: Sequence, d_out: Sequence, d_proj: Sequence, d_trans: Sequence,\n",
    "                 m: torch.nn.Module, s: Sequence[torch.nn.Module], direct_pairs: Sequence[tuple] = None):\n",
    "        \"\"\" Create a LatentRegModel object.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            d_in: d_in[g] gives the input dimensionality for group g of input variables.\n",
    "\n",
    "            d_out: d_out[h] gives the output dimensionality for group h of output variables.\n",
    "\n",
    "            d_proj: d_proj[g] gives the dimensionality for the projected latent variables for input group g.\n",
    "\n",
    "            d_trans: d_trans[h] gives the dimensionality for the transformed latent variables for output group h.\n",
    "\n",
    "            m: The mapping from [p_1, ..., p_G] to [t_h, ..., t_h].\n",
    "\n",
    "            s: s[h] contains module to be applied to o_h (see above).\n",
    "\n",
    "            direct_pairs: direct_pairs[p] contains a tuple of the form (g, h) giving a pair of input and output groups\n",
    "            that should have direct connections.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize projection matrices down\n",
    "        n_input_groups = len(d_in)\n",
    "        self.n_input_groups = n_input_groups\n",
    "        p = [None]*n_input_groups\n",
    "        for g, dims in enumerate(zip(d_in, d_proj)):\n",
    "            param_name = 'p' + str(g)\n",
    "            p[g] = torch.nn.Parameter(torch.zeros([dims[0], dims[1]]), requires_grad=True)\n",
    "            torch.nn.init.xavier_normal_(p[g])\n",
    "            self.register_parameter(param_name, p[g])\n",
    "        self.p = p\n",
    "\n",
    "        # Initialize projection matrices up\n",
    "        n_output_groups = len(d_out)\n",
    "        self.n_output_groups = n_output_groups\n",
    "        u = [None]*n_output_groups\n",
    "        for h, dims in enumerate(zip(d_out, d_trans)):\n",
    "            param_name = 'u' + str(h)\n",
    "            u[h] = torch.nn.Parameter(torch.zeros([dims[0], dims[1]]), requires_grad=True)\n",
    "            torch.nn.init.xavier_normal_(u[h])\n",
    "            self.register_parameter(param_name, u[h])\n",
    "        self.u = u\n",
    "\n",
    "        # Mapping from projection to transformed latents\n",
    "        self.m = m\n",
    "\n",
    "        # Direct mappings - there are none, we set direct_mappings to None\n",
    "        if direct_pairs is not None:\n",
    "            n_direct_pairs = len(direct_pairs)\n",
    "            direct_mappings = [None]*n_direct_pairs\n",
    "            for pair_i, pair in enumerate(direct_pairs):\n",
    "                c = torch.nn.Parameter(torch.ones(d_in[pair[0]]), requires_grad=True)\n",
    "                torch.nn.init.normal_(c, 0, .1)\n",
    "                param_name = 'c' + str(pair[0]) + '_' + str(pair[1])\n",
    "                self.register_parameter(param_name, c)\n",
    "                direct_mappings[pair_i] = {'pair': pair, 'c': c}\n",
    "            self.direct_mappings = direct_mappings\n",
    "        else:\n",
    "            self.direct_mappings = None\n",
    "\n",
    "        # Mappings from transformed latents to means\n",
    "        self.s = torch.nn.ModuleList(s)\n",
    "\n",
    "        # Initialize the variances for the noise variables\n",
    "        psi = [None]*n_output_groups\n",
    "        for h, d in enumerate(d_out):\n",
    "            param_name = 'psi' + str(h)\n",
    "            psi[h] = torch.nn.Parameter(torch.zeros(d), requires_grad=True)\n",
    "            torch.nn.init.uniform_(psi[h], .01, .02)\n",
    "            self.register_parameter(param_name, psi[h])\n",
    "        self.psi = psi\n",
    "\n",
    "    def forward(self, x: Sequence) -> Sequence:\n",
    "        \"\"\" Computes the predicted mean from the model given input.\n",
    "\n",
    "        Args:\n",
    "            x: A sequence of inputs.  x[g] contains the input tensor for group g.  x[g] should be of\n",
    "            shape n_smps*d_in[g]\n",
    "\n",
    "        Returns:\n",
    "            y: A sequence of outputs. y[h] contains the output for group h.  y[h] will be of shape n_smps*d_out[h]\n",
    "        \"\"\"\n",
    "\n",
    "        proj = [torch.matmul(x_g, p_g) for x_g, p_g in zip(x, self.p)]\n",
    "        #tran = self.m(proj)\n",
    "        tran = proj\n",
    "        z = [torch.matmul(t_h, u_h.t()) for t_h, u_h in zip(tran, self.u)]\n",
    "\n",
    "        mn = [s_h(z_h) for z_h, s_h in zip(z, self.s)]\n",
    "\n",
    "        return mn\n",
    "\n",
    "    def generate(self, x: Sequence) -> Sequence:\n",
    "        \"\"\" Generates outputs from the model given inputs.\n",
    "\n",
    "        Args:\n",
    "            x: A sequence of inputs.  x[g] contains the input tensor for group g.  x[g] should be of\n",
    "            shape n_smps*d_in[g]\n",
    "\n",
    "        Returns:\n",
    "            y: A sequence of generated outputs.  y[h] contains the output tensor for group h.  y[h] will be of\n",
    "            shape n_smps*d_out[h]\n",
    "        \"\"\"\n",
    "\n",
    "        n_output_grps = len(self.psi)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mns = self(x)\n",
    "            y = [None]*n_output_grps\n",
    "            for h in range(n_output_grps):\n",
    "                noise_h = torch.randn_like(mns[h])*torch.sqrt(self.psi[h])\n",
    "                y[h] = mns[h] + noise_h\n",
    "\n",
    "        return y\n",
    "\n",
    "    def neg_ll(self, y: Sequence, mn: Sequence):\n",
    "\n",
    "        \"\"\"\n",
    "        Calculates the negative log likelihood of outputs given predicted means.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            y: A sequence of outputs.  y[h] contains the output tensor for group h.  y[h] should be of\n",
    "            shape n_smps*d_out[h]\n",
    "\n",
    "            mns: A sequence of predicted means.  mns[h] contains the predicted means for group h.  mns[h]\n",
    "            should be of shape n_smps*d_out[h]\n",
    "\n",
    "        Returns:\n",
    "            The calculated negative log-likelihood for the sample\n",
    "        \"\"\"\n",
    "\n",
    "        #neg_ll = float(0)\n",
    "\n",
    "        #n_smps = y[0].shape[0]\n",
    "        #neg_log_2_pi = float(np.log(2*np.pi))\n",
    "\n",
    "        #for mn_h, y_h, psi_h in zip(mn, y, self.psi):\n",
    "        #    neg_ll += .5*mn_h.nelement()*neg_log_2_pi\n",
    "        #    neg_ll += .5*n_smps*torch.sum(torch.log(psi_h))\n",
    "        #    neg_ll += .5*torch.sum(((y_h - mn_h)**2)/psi_h)\n",
    "\n",
    "        neg_ll = .5*torch.sum(((y[0] - mn[0])**2))\n",
    "        \n",
    "        return neg_ll\n",
    "\n",
    "    def fit(self, x: torch.Tensor, y: torch.Tensor, batch_size: int=100, send_size: int=100, max_its: int=10,\n",
    "            learning_rates=.01, adam_params: dict = {}, min_var: float = 0.0, update_int: int = 1000,\n",
    "            parameters: list = None):\n",
    "\n",
    "        \"\"\" Fits a model to data.\n",
    "\n",
    "        This function performs stochastic optimization with the ADAM algorithm.  The weights of the model\n",
    "        should be initialized before calling this function.\n",
    "\n",
    "        Optimization will be perfomed on whatever device the model parameters are on.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            x: A sequence of inputs.  x[g] contains the input tensor for group g.  x[g] should be of\n",
    "            shape n_smps*d_in[g]\n",
    "\n",
    "            y: A sequence of outputs.  y[h] contains the output tensor for group h.  y[h] should be of\n",
    "            shape n_smps*d_out[h]\n",
    "\n",
    "            batch_size: The number of samples to train on during each iteration\n",
    "\n",
    "            send_size: The number of samples to send to the device at a time for calculating batch gradients.  It is\n",
    "            most efficient to set send_size = batch_size, but if this results in computations exceeding device memory,\n",
    "            send_size can be set lower.  In this case gradients will accumulated until all samples in the batch are\n",
    "            sent to the device and then a step will be taken.\n",
    "\n",
    "            max_its: The maximum number of iterations to run\n",
    "\n",
    "            learning_rates: If a single number, this is the learning rate to use for all iteration.  Alternatively, this\n",
    "            can be a list of tuples.  Each tuple is of the form (iteration, learning_rate), which gives the learning rate\n",
    "            to use from that iteration onwards, until another tuple specifies another learning rate to use at a different\n",
    "            iteration on.  E.g., learning_rates = [(0, .01), (1000, .001), (10000, .0001)] would specify a learning\n",
    "            rate of .01 from iteration 0 to 999, .001 from iteration 1000 to 9999 and .0001 from iteration 10000 onwards.\n",
    "\n",
    "            adam_params: Dictionary of parameters to pass to the call when creating the Adam Optimizer object.\n",
    "            Note that if learning rate is specified here *it will be ignored.* (Use the learning_rates option instead).\n",
    "\n",
    "            min_var: The minumum value any entry of a psi[h] can take on.  After a gradient update, values less than this\n",
    "            will be clamped to this value.\n",
    "\n",
    "            update_int: The interval of iterations we update the user on.\n",
    "\n",
    "            parameters: If provided, only these parameters of the model will be optimized.  If none, all parameters are\n",
    "            optimized.\n",
    "\n",
    "            Raises:\n",
    "                ValueError: If send_size is greater than batch_size.\n",
    "\n",
    "            Returns:\n",
    "                log: A dictionary logging progress.  Will have the enries:\n",
    "                'elapsed_time': log['elapsed_time'][i] contains the elapsed time from the beginning of optimization to\n",
    "                the end of iteration i\n",
    "\n",
    "                'obj': log['obj'][i] contains the objective value at the beginning (before parameters are updated) of iteration i.\n",
    "\n",
    "    \"\"\"\n",
    "        if send_size > batch_size:\n",
    "            raise (ValueError('send_size must be less than or equal to batch_size.'))\n",
    "\n",
    "        device = self.p[0].device\n",
    "\n",
    "        if parameters is None:\n",
    "            parameters = self.parameters()\n",
    "        # Convert generator to list (since we need to reference parameters multiple times in the code below)\n",
    "        parameters = [p for p in parameters]\n",
    "\n",
    "        if not isinstance(learning_rates, (int, float, list)):\n",
    "            raise (ValueError('learning_rates must be of type int, float or list.'))\n",
    "\n",
    "        # Format and check learning rates - no matter the input format this outputs learning rates in a standard format\n",
    "        # where the learning rate starting at iteration 0 is guaranteed to be listed first\n",
    "        learning_rate_its, learning_rate_values = format_and_check_learning_rates(learning_rates)\n",
    "\n",
    "        optimizer = torch.optim.Adam(parameters, lr=learning_rate_values[0], **adam_params)\n",
    "\n",
    "        n_smps = x[0].shape[0]\n",
    "        cur_it = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        elapsed_time_log = np.zeros(max_its)\n",
    "        obj_log = np.zeros(max_its)\n",
    "        prev_learning_rate = learning_rate_values[0]\n",
    "\n",
    "        while cur_it < max_its:\n",
    "            elapsed_time = time.time() - start_time  # Record elapsed time here because we measure it from the start of\n",
    "            # each iteration.  This is because we also record the nll value for each iteration before parameters are\n",
    "            # updated.  In this way, the elapsed time is the elapsed time to get to a set of parameters for which we\n",
    "            # report the nll.\n",
    "\n",
    "            # Set the learning rate\n",
    "            cur_learing_rate_ind = np.nonzero(learning_rate_its <= cur_it)[0]\n",
    "            cur_learing_rate_ind = cur_learing_rate_ind[-1]\n",
    "            cur_learning_rate = learning_rate_values[cur_learing_rate_ind]\n",
    "            if cur_learning_rate != prev_learning_rate:\n",
    "                # We reset the whole optimizer because ADAM is an adaptive optimizer\n",
    "                optimizer = torch.optim.Adam(parameters, lr=cur_learning_rate, **adam_params)\n",
    "                prev_learning_rate = cur_learning_rate\n",
    "\n",
    "            # Chose the samples for this iteration: TODO: Move generation of random samples out of the loop\n",
    "            cur_smps = np.random.choice(n_smps, batch_size, replace=False)\n",
    "            batch_x = [x_g[cur_smps, :] for x_g in x]\n",
    "            batch_y = [y_h[cur_smps, :] for y_h in y]\n",
    "\n",
    "            # Perform optimization for this step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Handle sending data to device in small chunks if needed\n",
    "            start_ind = 0\n",
    "            end_ind = np.min([batch_size, send_size])\n",
    "            while True:\n",
    "                sent_x = [batch_x_g[start_ind:end_ind, :].to(device) for batch_x_g in batch_x]\n",
    "                sent_y = [batch_y_h[start_ind:end_ind, :].to(device) for batch_y_h in batch_y]\n",
    "\n",
    "                mns = self(sent_x)\n",
    "                # Calculate nll - we divide by batch size to get average (over samples) negative log-likelihood\n",
    "                obj = (1 / batch_size) * self.neg_ll(sent_y, mns)\n",
    "                obj.backward()\n",
    "                \n",
    "            #sent_x = batch_x\n",
    "            #sent_y = batch_y\n",
    "            #mns = self(sent_x)\n",
    "            #obj = (1 / batch_size) * self.neg_ll(sent_y, mns)\n",
    "            #obj.backward()\n",
    "\n",
    "                if end_ind == batch_size:\n",
    "                    break\n",
    "\n",
    "                start_end = end_ind\n",
    "                end_ind = np.min([batch_size, start_end + send_size])\n",
    "\n",
    "            # Take a step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Correct any noise variances that are too small\n",
    "            with torch.no_grad():\n",
    "                for psi_h in self.psi:\n",
    "                    small_psi_inds = torch.nonzero(psi_h < min_var)\n",
    "                    psi_h.data[small_psi_inds] = min_var\n",
    "\n",
    "            # Log our progress\n",
    "            elapsed_time_log[cur_it] = elapsed_time\n",
    "            obj_vl = obj.cpu().detach().numpy()\n",
    "            obj_log[cur_it] = obj_vl\n",
    "\n",
    "            # Provide user with some feedback\n",
    "            if cur_it % update_int == 0:\n",
    "                print(str(cur_it) + ': Elapsed fitting time ' + str(elapsed_time) +\n",
    "                      ', vl: ' + str(obj_vl) + ', lr: ' + str(cur_learning_rate))\n",
    "\n",
    "            cur_it += 1\n",
    "\n",
    "        # Give final fitting results (if we have not already)\n",
    "        if update_int != 1:\n",
    "            print(str(cur_it - 1) + ': Elapsed fitting time ' + str(elapsed_time) +\n",
    "                    ', vl: ' + str(obj_vl))\n",
    "\n",
    "        log = {'elapsed_time': elapsed_time_log, 'obj': obj_log}\n",
    "\n",
    "        return log\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for model go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = [1000, 1000]\n",
    "d_out = [1000, 1000]\n",
    "\n",
    "d_proj = [1, 1]\n",
    "d_trans = [1, 1]\n",
    "\n",
    "n_smps = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create true model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the model here\n",
    "n_output_groups = len(d_in)\n",
    "\n",
    "M = IdentityMap()\n",
    "S = [Bias(d_o) for d_o in d_out]\n",
    "mdl = LatentRegModel(d_in, d_out, d_proj, d_trans, M, S, direct_pairs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [torch.randn([n_smps, d]) for d in d_in]\n",
    "y_pred = mdl(x)\n",
    "y = mdl.generate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_fitted = IdentityMap()\n",
    "S_fitted = [Bias(d_o) for d_o in d_out]\n",
    "fitted_mdl = LatentRegModel(d_in, d_out, d_proj, d_trans, M_fitted, S_fitted, direct_pairs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_mdl=fitted_mdl.cuda()\n",
    "x = [x_i.to('cuda') for x_i in x]\n",
    "y = [y_i.to('cuda') for y_i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Elapsed fitting time 0.0, vl: 21443.697, lr: 0.01\n",
      "10: Elapsed fitting time 0.3281431198120117, vl: 20824.525, lr: 0.01\n",
      "20: Elapsed fitting time 0.40627098083496094, vl: 20502.53, lr: 0.01\n",
      "30: Elapsed fitting time 0.48703598976135254, vl: 20017.59, lr: 0.01\n",
      "40: Elapsed fitting time 0.566779613494873, vl: 19038.352, lr: 0.01\n",
      "50: Elapsed fitting time 0.644906759262085, vl: 18967.07, lr: 0.01\n",
      "60: Elapsed fitting time 0.7125897407531738, vl: 18423.828, lr: 0.01\n",
      "70: Elapsed fitting time 0.7907516956329346, vl: 17997.465, lr: 0.01\n",
      "80: Elapsed fitting time 0.8688795566558838, vl: 17805.064, lr: 0.01\n",
      "90: Elapsed fitting time 0.939685583114624, vl: 17522.883, lr: 0.01\n",
      "100: Elapsed fitting time 1.0113561153411865, vl: 16951.727, lr: 0.01\n",
      "110: Elapsed fitting time 1.073850393295288, vl: 16461.814, lr: 0.01\n",
      "120: Elapsed fitting time 1.160318374633789, vl: 15799.961, lr: 0.01\n",
      "130: Elapsed fitting time 1.2322306632995605, vl: 15590.333, lr: 0.01\n",
      "140: Elapsed fitting time 1.2947325706481934, vl: 15077.685, lr: 0.01\n",
      "150: Elapsed fitting time 1.3772087097167969, vl: 15192.9, lr: 0.01\n",
      "160: Elapsed fitting time 1.4532437324523926, vl: 14667.74, lr: 0.01\n",
      "170: Elapsed fitting time 1.5157480239868164, vl: 14235.291, lr: 0.01\n",
      "180: Elapsed fitting time 1.5938928127288818, vl: 14154.307, lr: 0.01\n",
      "190: Elapsed fitting time 1.6588377952575684, vl: 13794.335, lr: 0.01\n",
      "200: Elapsed fitting time 1.7369399070739746, vl: 13564.991, lr: 0.01\n",
      "210: Elapsed fitting time 1.8150622844696045, vl: 13299.356, lr: 0.01\n",
      "220: Elapsed fitting time 1.8811874389648438, vl: 12751.364, lr: 0.01\n",
      "230: Elapsed fitting time 1.9593193531036377, vl: 12444.773, lr: 0.01\n",
      "240: Elapsed fitting time 2.048224687576294, vl: 12334.544, lr: 0.01\n",
      "250: Elapsed fitting time 2.132044553756714, vl: 11794.585, lr: 0.01\n",
      "260: Elapsed fitting time 2.19454288482666, vl: 11607.786, lr: 0.01\n",
      "270: Elapsed fitting time 2.277644157409668, vl: 11293.816, lr: 0.01\n",
      "280: Elapsed fitting time 2.340178966522217, vl: 11038.342, lr: 0.01\n",
      "290: Elapsed fitting time 2.4183080196380615, vl: 10959.275, lr: 0.01\n",
      "300: Elapsed fitting time 2.495028495788574, vl: 10430.633, lr: 0.01\n",
      "310: Elapsed fitting time 2.573190212249756, vl: 10471.582, lr: 0.01\n",
      "320: Elapsed fitting time 2.6513187885284424, vl: 10243.599, lr: 0.01\n",
      "330: Elapsed fitting time 2.7315542697906494, vl: 9823.112, lr: 0.01\n",
      "340: Elapsed fitting time 2.809676170349121, vl: 9762.721, lr: 0.01\n",
      "350: Elapsed fitting time 2.8878121376037598, vl: 9268.546, lr: 0.01\n",
      "360: Elapsed fitting time 2.962235450744629, vl: 9262.41, lr: 0.01\n",
      "370: Elapsed fitting time 3.0288844108581543, vl: 8891.681, lr: 0.01\n",
      "380: Elapsed fitting time 3.1226367950439453, vl: 9076.217, lr: 0.01\n",
      "390: Elapsed fitting time 3.200385808944702, vl: 8694.09, lr: 0.01\n",
      "400: Elapsed fitting time 3.278513193130493, vl: 8433.92, lr: 0.01\n",
      "410: Elapsed fitting time 3.3410115242004395, vl: 8166.955, lr: 0.01\n",
      "420: Elapsed fitting time 3.41597580909729, vl: 7992.567, lr: 0.01\n",
      "430: Elapsed fitting time 3.5097615718841553, vl: 7950.003, lr: 0.01\n",
      "440: Elapsed fitting time 3.587904691696167, vl: 7654.387, lr: 0.01\n",
      "450: Elapsed fitting time 3.652719736099243, vl: 7596.3677, lr: 0.01\n",
      "460: Elapsed fitting time 3.7465054988861084, vl: 7540.3857, lr: 0.01\n",
      "470: Elapsed fitting time 3.809009075164795, vl: 7273.7515, lr: 0.01\n",
      "480: Elapsed fitting time 3.8895437717437744, vl: 7246.0913, lr: 0.01\n",
      "490: Elapsed fitting time 3.9676713943481445, vl: 6853.844, lr: 0.01\n",
      "500: Elapsed fitting time 4.04580020904541, vl: 6731.118, lr: 0.01\n",
      "510: Elapsed fitting time 4.125063180923462, vl: 6592.8228, lr: 0.01\n",
      "520: Elapsed fitting time 4.203223466873169, vl: 6381.413, lr: 0.01\n",
      "530: Elapsed fitting time 4.281352281570435, vl: 6404.136, lr: 0.01\n",
      "540: Elapsed fitting time 4.356072902679443, vl: 6108.1724, lr: 0.01\n",
      "550: Elapsed fitting time 4.425140619277954, vl: 6101.3306, lr: 0.01\n",
      "560: Elapsed fitting time 4.503268718719482, vl: 5841.539, lr: 0.01\n",
      "570: Elapsed fitting time 4.584357500076294, vl: 5785.2812, lr: 0.01\n",
      "580: Elapsed fitting time 4.662521600723267, vl: 5608.8174, lr: 0.01\n",
      "590: Elapsed fitting time 4.725022554397583, vl: 5442.6367, lr: 0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d806c5e6b7b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitted_mdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_its\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msend_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-43bcab080c3e>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, send_size, max_its, learning_rates, adam_params, min_var, update_int, parameters)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;31m# Chose the samples for this iteration: TODO: Move generation of random samples out of the loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mcur_smps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_smps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_g\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_smps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx_g\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_h\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_smps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_h\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-43bcab080c3e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    283\u001b[0m             \u001b[1;31m# Chose the samples for this iteration: TODO: Move generation of random samples out of the loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[0mcur_smps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_smps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_g\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_smps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx_g\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_h\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcur_smps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_h\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log = fitted_mdl.fit(x, y, max_its=10000, batch_size=1000, send_size = 1000, update_int=10, min_var = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
