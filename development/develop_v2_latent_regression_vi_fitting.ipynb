{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for development and testing of code for the second version of fitting latent regression models across multiple subjects with variational inference.  The main advance in version 2.0 of the code is the ability to support distributions across additional model parameters (not just the modes). \n",
    "\n",
    "In particular we generate models of how one neural population drives another as follows:\n",
    "\n",
    "1) The user specified a number of subjects and how many neurons are in each population for each of those subjects. Neuron locations for each subject are than randomly drawn from a uniform distribution on the unit square. \n",
    "\n",
    "2) Our models include only neural dynamics (no stimulus input or behavioral output) and we use an identity mapping in \n",
    "the low d space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from janelia_core.ml.datasets import TimeSeriesDataset\n",
    "from janelia_core.ml.latent_regression.group_maps import GroupLinearTransform, IdentityMap\n",
    "from janelia_core.ml.latent_regression.subject_models import LatentRegModel, SharedMLatentRegModel\n",
    "from janelia_core.ml.latent_regression.vi import MultiSubjectVIFitter\n",
    "from janelia_core.ml.latent_regression.vi import PriorCollection\n",
    "from janelia_core.ml.latent_regression.vi import SubjectVICollection\n",
    "from janelia_core.ml.latent_regression.vi import predict_with_truth\n",
    "from janelia_core.ml.torch_distributions import CondGaussianDistribution\n",
    "from janelia_core.ml.torch_distributions import CondMatrixHypercubePrior\n",
    "from janelia_core.ml.torch_distributions import CondMatrixProductDistribution\n",
    "from janelia_core.ml.torch_distributions import MatrixGaussianProductDistribution\n",
    "from janelia_core.ml.torch_parameter_penalizers import ScalarPenalizer\n",
    "from janelia_core.ml.utils import torch_mod_to_fcn\n",
    "from janelia_core.ml.utils import list_torch_devices\n",
    "from janelia_core.visualization.image_generation import generate_dot_image_3d\n",
    "from janelia_core.visualization.image_visualization import visualize_2d_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and model specification goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we specify the number of subjects (by the length of the list) and number of neurons that will be present\n",
    "# each population for each subject\n",
    "\n",
    "n_subj_neurons = [(10000, 10000),\n",
    "                  (9000, 9000),\n",
    "                  (11000, 11000)]\n",
    "\n",
    "# Number of samples of data to generate for each subject\n",
    "n_smps = 20000\n",
    "\n",
    "# True if we should used shared posteriors among subjects\n",
    "use_shared_posts = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for creating hypercube functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_fcn_params = {'n_divisions_per_dim': [50, 50], \n",
    "                 'dim_ranges': np.asarray([[-.1, 1.1], [-.1, 1.1]]), \n",
    "                 'n_div_per_hc_side_per_dim': [1, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we specify the mean and standard deviation functions for the different parameters of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class exp2d(torch.nn.Module):\n",
    "    def __init__(self, ctr, std, gain, offset):\n",
    "        #assert(ctr.shape == [1, 2])\n",
    "        #assert(std.shape == [1,2])\n",
    "        \n",
    "        super().__init__()\n",
    "        self.ctr = torch.nn.Parameter(ctr)\n",
    "        self.std = torch.nn.Parameter(std)\n",
    "        self.gain = torch.nn.Parameter(gain)\n",
    "        self.offset = torch.nn.Parameter(offset)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return (self.gain*torch.exp(-1*torch.sum((x - self.ctr)**2/self.std, dim=1)) + self.offset).unsqueeze(1)  \n",
    "\n",
    "class constantF(torch.nn.Module):\n",
    "    def __init__(self, vl):\n",
    "        super().__init__()\n",
    "        self.vl = vl\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.vl*torch.ones([x.shape[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the distributions over p and u modes\n",
    "\n",
    "Here we implicitly define the number of modes by the number of distributions we define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ctrs = [torch.tensor([.1, .1]), torch.tensor([.9, .9])]\n",
    "true_p_dists = CondMatrixProductDistribution([CondGaussianDistribution(mn_f=exp2d(ctr = c, \n",
    "                                                                                  std = torch.tensor([1.0, 1.0]),\n",
    "                                                                                  gain = torch.tensor(1.0), \n",
    "                                                                                  offset = torch.tensor(0.0)),\n",
    "                                                                        std_f=constantF(.1)) \n",
    "                                              for c in p_ctrs])\n",
    "\n",
    "\n",
    " \n",
    "u_ctrs = [torch.tensor([.1, .1]), torch.tensor([.9, .9])]\n",
    "true_u_dists = CondMatrixProductDistribution([CondGaussianDistribution(mn_f=exp2d(ctr = c, \n",
    "                                                                                  std = torch.tensor([1.0, 1.0]),\n",
    "                                                                                  gain = torch.tensor(1.0), \n",
    "                                                                                  offset = torch.tensor(0.0)),\n",
    "                                                                        std_f=constantF(.1)) \n",
    "                                              for c in u_ctrs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the distributions over scales and offsets and direct connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_scale_dist = CondGaussianDistribution(mn_f=exp2d(ctr = torch.tensor([.5, .5]), \n",
    "                                                    std = torch.tensor([.5, .5]),\n",
    "                                                    gain = torch.tensor(10.0), \n",
    "                                                    offset = torch.tensor(0.0)),\n",
    "                                        std_f=constantF(.1))\n",
    "\n",
    "true_offset_dist = CondGaussianDistribution(mn_f=exp2d(ctr = torch.tensor([.5, .5]), \n",
    "                                                    std = torch.tensor([1.0, 1.0]),\n",
    "                                                    gain = torch.tensor(10.0), \n",
    "                                                    offset = torch.tensor(0.0)),\n",
    "                                        std_f=constantF(.1))\n",
    "\n",
    "true_psi_dist = CondGaussianDistribution(mn_f=exp2d(ctr = torch.tensor([.5, .5]), \n",
    "                                                    std = torch.tensor([6.0, 6.0]),\n",
    "                                                    gain = torch.tensor(.2), \n",
    "                                                    offset = torch.tensor(.1)),\n",
    "                                        std_f=constantF(.01))\n",
    "\n",
    "true_direct_map_dist = CondGaussianDistribution(mn_f=exp2d(ctr = torch.tensor([.8, .5]), \n",
    "                                                    std = torch.tensor([6.0, 6.0]),\n",
    "                                                    gain = torch.tensor(.2), \n",
    "                                                    offset = torch.tensor(.1)),\n",
    "                                        std_f=constantF(.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we generate our true subject models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_modes = len(true_p_dists.dists)\n",
    "n_subjs = len(n_subj_neurons)\n",
    "true_subj_models = [None]*n_subjs\n",
    "true_data = [None]*n_subjs\n",
    "\n",
    "for s_i in range(n_subjs):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Generate neuron locations\n",
    "        p_neuron_locs = torch.rand(size=[n_subj_neurons[s_i][0], 2])\n",
    "        u_neuron_locs = torch.rand(size=[n_subj_neurons[s_i][1], 2])\n",
    "    \n",
    "        # Generate modes\n",
    "        p_modes = true_p_dists.form_standard_sample(true_p_dists.sample(p_neuron_locs))\n",
    "        u_modes = true_u_dists.form_standard_sample(true_u_dists.sample(u_neuron_locs))\n",
    "        \n",
    "        # Generate scales and offsets\n",
    "        scales = true_scale_dist.form_standard_sample(true_scale_dist.sample(u_neuron_locs)).squeeze()\n",
    "        offsets = true_offset_dist.form_standard_sample(true_offset_dist.sample(u_neuron_locs)).squeeze()\n",
    "        \n",
    "        # Generate direct maps\n",
    "        direct_mappings = true_direct_map_dist.form_standard_sample(true_direct_map_dist.sample(u_neuron_locs)).squeeze()\n",
    "        \n",
    "        # Generate psi\n",
    "        psi = true_psi_dist.form_standard_sample(true_psi_dist.sample(u_neuron_locs)).squeeze()\n",
    "        assert(torch.all(psi > 0))\n",
    "    \n",
    "        s_mdl = LatentRegModel(d_in = [n_subj_neurons[s_i][0]], d_out = [n_subj_neurons[s_i][1]], \n",
    "                               d_proj=[n_modes], d_trans=[n_modes], \n",
    "                               m=IdentityMap(),\n",
    "                               s=[torch.nn.Identity()], \n",
    "                               use_scales=True,\n",
    "                               use_offsets=True,\n",
    "                               direct_pairs=[(0,0)], \n",
    "                               assign_direct_pair_mappings=True)\n",
    "    \n",
    "        s_mdl.u[0].data = u_modes\n",
    "        s_mdl.p[0].data = p_modes\n",
    "        s_mdl.offsets[0].data = offsets\n",
    "        s_mdl.scales[0].data = scales\n",
    "        s_mdl.psi[0].data = psi\n",
    "        s_mdl.direct_mappings[0].data = direct_mappings\n",
    "    \n",
    "        true_subj_models[s_i] = {'mdl': s_mdl, 'p_neuron_locs': p_neuron_locs, 'u_neuron_locs': u_neuron_locs}\n",
    "    \n",
    "        \n",
    "        p_data = [torch.randn(size=[n_smps, n_subj_neurons[s_i][0]])]\n",
    "        u_data = s_mdl.generate(p_data)\n",
    "        \n",
    "        # Delay u data with respect to u data (since we model u_{t+1} as a function of p_t)\n",
    "        p_data[0] = p_data[0][1:,:]\n",
    "        u_data[0] = u_data[0][0:-1, :]\n",
    "        \n",
    "        \n",
    "        \n",
    "        true_data[s_i] = (p_data, u_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we set things up for fitting with variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_prior = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, std_hc_params=hc_fcn_params, \n",
    "                                   min_std=.00001)\n",
    "\n",
    "u_prior = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, std_hc_params=hc_fcn_params, \n",
    "                                   min_std=.00001)\n",
    "\n",
    "scales_prior = CondMatrixHypercubePrior(n_cols=1, mn_hc_params=hc_fcn_params, std_hc_params=hc_fcn_params, \n",
    "                                   min_std=.00001, mn_init=1.0)\n",
    "\n",
    "offsets_prior = CondMatrixHypercubePrior(n_cols=1, mn_hc_params=hc_fcn_params, std_hc_params=hc_fcn_params, \n",
    "                                   min_std=.00001, mn_init=0.0)\n",
    "\n",
    "direct_mappings_prior = CondMatrixHypercubePrior(n_cols=1, mn_hc_params=hc_fcn_params, std_hc_params=hc_fcn_params, \n",
    "                                   min_std=.00001, mn_init=0.0)\n",
    "\n",
    "prior_collection = PriorCollection(p_dists=[p_prior], u_dists=[u_prior], psi_dists=[None], \n",
    "                                   scale_dists=[scales_prior], offset_dists=[offsets_prior], \n",
    "                                   direct_mapping_dists=[direct_mappings_prior])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define subject models and posteriors for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_collections = [None]*n_subjs\n",
    "for s_i in range(n_subjs):\n",
    "    \n",
    "    # Create subject model for fitting\n",
    "    subject_specific_m = GroupLinearTransform(d=[n_modes], nonnegative_scale=True, \n",
    "                                              v_mn=1.0, v_std=.01, o_mn=0.0, o_std=.01)\n",
    "    s_mdl = SharedMLatentRegModel(d_in = [n_subj_neurons[s_i][0]], d_out = [n_subj_neurons[s_i][1]], \n",
    "                                  d_proj=[n_modes], d_trans=[n_modes], specific_m=subject_specific_m,\n",
    "                                  shared_m=IdentityMap(), s=[torch.nn.Identity()],\n",
    "                                  use_scales=True, use_offsets=True, direct_pairs=[(0,0)],\n",
    "                                  assign_p_modes=False, assign_u_modes=False, assign_scales=False, assign_offsets=False,\n",
    "                                  assign_direct_pair_mappings=False,\n",
    "                                  assign_psi=True) # We will fit point estimates for psi (and not distributions)    \n",
    "    \n",
    "    # Create posterior distributions \n",
    "    if use_shared_posts:\n",
    "        if s_i == 0:\n",
    "            p_post = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, \n",
    "                                              std_hc_params=hc_fcn_params, min_std=.00001, \n",
    "                                              mn_init=.1)\n",
    "            u_post = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, \n",
    "                                              std_hc_params=hc_fcn_params, min_std=.00001,\n",
    "                                              mn_init=.1)\n",
    "            scale_post = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, \n",
    "                                                  std_hc_params=hc_fcn_params, min_std=.00001, \n",
    "                                                  mn_init=1.0)\n",
    "            offset_post = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, \n",
    "                                                   std_hc_params=hc_fcn_params, min_std=.00001,\n",
    "                                                   mn_init=0.0)\n",
    "            direct_mappings_post = CondMatrixHypercubePrior(n_cols=n_modes, mn_hc_params=hc_fcn_params, \n",
    "                                                            std_hc_params=hc_fcn_params, min_std=.00001,\n",
    "                                                            mn_init=0.0)\n",
    "        else:\n",
    "            pass # Do nothing, we can just keep using the posteriors we already created for subject 1\n",
    "    else:\n",
    "        p_post = MatrixGaussianProductDistribution(shape=[n_subj_neurons[s_i][0], n_modes], mn_mn=.01, mn_std=.001)\n",
    "        u_post = MatrixGaussianProductDistribution(shape=[n_subj_neurons[s_i][1], n_modes], mn_mn=.01, mn_std=.001)\n",
    "        scale_post = MatrixGaussianProductDistribution(shape=[n_subj_neurons[s_i][1], 1], mn_mn=1.0, mn_std=.001)\n",
    "        offset_post = MatrixGaussianProductDistribution(shape=[n_subj_neurons[s_i][1], 1], mn_mn=0.0, mn_std=.001)\n",
    "        direct_mappings_post = MatrixGaussianProductDistribution(shape=[n_subj_neurons[s_i][1], 1], mn_mn=0.0, mn_std=.001)\n",
    "    \n",
    "    # Package data\n",
    "    data = TimeSeriesDataset([true_data[s_i][0][0], true_data[s_i][1][0]])[:]\n",
    "    \n",
    "    vi_collections[s_i] = SubjectVICollection(s_mdl=s_mdl, p_dists=[p_post], u_dists=[u_post], psi_dists=[None],\n",
    "                                        scale_dists=[scale_post], \n",
    "                                        offset_dists=[offset_post],\n",
    "                                        direct_mappings_dists=[direct_mappings_post],\n",
    "                                        data=data, input_grps=[0], output_grps=[1], \n",
    "                                        props=[true_subj_models[s_i]['p_neuron_locs'], \n",
    "                                               true_subj_models[s_i]['u_neuron_locs']],\n",
    "                                        p_props = [0], u_props=[1], psi_props=[None], \n",
    "                                        scale_props=[1], offset_props=[1], \n",
    "                                        direct_mapping_props=[1], min_var=[.0001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate penalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_v_params = [coll.s_mdl.specific_m.v[0] for coll in vi_collections]\n",
    "v_penalizer = ScalarPenalizer(params=subj_v_params, w=10000000.0, init_ctr=1.0, learnable_parameters=False, \n",
    "                              description='m scales')\n",
    "subj_o_params = [coll.s_mdl.specific_m.o[0] for coll in vi_collections]\n",
    "o_penalizer = ScalarPenalizer(params=subj_o_params, w=10000000.0, init_ctr=0.0, learnable_parameters=False, \n",
    "                              description='m offsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the fitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter = MultiSubjectVIFitter(s_collections=vi_collections, prior_collection=prior_collection,\n",
    "                              penalizers=[v_penalizer, o_penalizer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs found.\n"
     ]
    }
   ],
   "source": [
    "devices, _ = list_torch_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.distribute(devices, distribute_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bishopw/Documents/Janelia_Research/Projects/janelia_core/janelia_core/ml/latent_regression/vi.py:1018: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple) (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1595629449223/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  small_psi_inds = torch.nonzero(s_mdl.psi[h] < s_min_var[h])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************\n",
      "Epoch 0 complete.  Obj: 5.30e+14, LR: [0.1 {'fast': 1}]\n",
      "Model NLLs:  s_0: 1.75e+14, s_1: 1.43e+14, s_2: 2.12e+14\n",
      "Subj P KLs:  s_0: 4.97e+05, s_1: 4.21e+05, s_2: 5.77e+05\n",
      "Subj U KLs:  s_0: 9.12e+03, s_1: 8.22e+03, s_2: 1.00e+04\n",
      "Subj Psi KLs:  s_0: 0.00e+00, s_1: 0.00e+00, s_2: 0.00e+00\n",
      "Subj Scale KLs:  s_0: 8.38e+05, s_1: 7.48e+05, s_2: 9.32e+05\n",
      "Subj Offsets KLs:  s_0: 7.99e+05, s_1: 7.19e+05, s_2: 8.87e+05\n",
      "Subj Direct Mappings KLs:  s_0: 8.10e+05, s_1: 7.29e+05, s_2: 8.62e+05\n",
      "Penalties:  p_0: 6.53e+05, p_1: 6.47e+05\n",
      "m scales state\n",
      " Center: [1.]\n",
      " Last Penalty: 652918.375\n",
      "m offsets state\n",
      " Center: [0.]\n",
      " Last Penalty: 646554.8125\n",
      "Device memory allocated:  d_0: nan\n",
      "Device max memory allocated:  d_0: nan\n",
      "Elapsed time: 14.35665512084961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d297f2055f45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m logs0 = fitter.fit(n_epochs=1000, n_batches=2, update_int=10, learning_rates=[(0, .1, {'fast': 1})], \n\u001b[0m\u001b[1;32m      2\u001b[0m                   enforce_priors=(use_shared_posts==False))\n\u001b[1;32m      3\u001b[0m logs1 = fitter.fit(n_epochs=1000, n_batches=2, update_int=10, learning_rates=.01, \n\u001b[1;32m      4\u001b[0m                   enforce_priors=(use_shared_posts==False))\n",
      "\u001b[0;32m~/Documents/Janelia_Research/Projects/janelia_core/janelia_core/ml/latent_regression/vi.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epochs, n_batches, learning_rates, adam_params, s_inds, fix_priors, enforce_priors, update_int, print_opts, cp_epochs, cp_penalizers)\u001b[0m\n\u001b[1;32m    953\u001b[0m                     nll = (float(n_smp_data_points[i])/n_batch_data_pts)*s_coll.s_mdl.neg_ll(y=batch_y, mn=y_pred,\n\u001b[1;32m    954\u001b[0m                                                                                              psi=q_psi_vls_standard)\n\u001b[0;32m--> 955\u001b[0;31m                     \u001b[0mnll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m                     \u001b[0mbatch_obj_log\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m                     \u001b[0mbatch_nll\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/janelia_core/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/janelia_core/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logs0 = fitter.fit(n_epochs=1000, n_batches=2, update_int=10, learning_rates=[(0, .1, {'fast': 1})], \n",
    "                  enforce_priors=(use_shared_posts==False))\n",
    "logs1 = fitter.fit(n_epochs=1000, n_batches=2, update_int=10, learning_rates=.01, \n",
    "                  enforce_priors=(use_shared_posts==False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.plot_log(logs0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move everything to cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitter.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at predictions the models make on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_preds = [predict_with_truth(s_coll, s_coll.data) for s_coll in vi_collections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_s_i = 2\n",
    "plot_v_i = 3\n",
    "smp_inds = slice(0, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(s_preds[plt_s_i]['truth'][0][smp_inds, plot_v_i], 'b-')\n",
    "plt.plot(s_preds[plt_s_i]['pred'][0][smp_inds, plot_v_i], 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at true and fit offset and scale distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offsets\n",
    "#true_dist = true_offset_dist \n",
    "#fit_dist = offsets_prior.dists[0] \n",
    "\n",
    "# Scales\n",
    "true_dist = true_scale_dist\n",
    "fit_dist = scales_prior.dists[0]\n",
    "\n",
    "# Direct mapings\n",
    "#true_dist = true_direct_map_dist\n",
    "#fit_dist = direct_mappings_prior.dists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "visualize_2d_function(torch_mod_to_fcn(true_dist.mn_f), ax=plt.subplot(1,2, 1))\n",
    "#plt.gca().get_images()[0].set_clim(0.0, 10.0)\n",
    "visualize_2d_function(torch_mod_to_fcn(fit_dist.mn_f), ax=plt.subplot(1,2, 2), \n",
    "                      dim_0_range=[0, .99], dim_1_range=[0, .99])\n",
    "#plt.gca().get_images()[0].set_clim(0.0, 10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at true and fit offset values compared to posteriors on a single neuron basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_s_i = 2\n",
    "e_shape = [21, 21, 1]\n",
    "\n",
    "# Offsets\n",
    "true_vls = true_subj_models[vis_s_i]['mdl'].offsets[0]\n",
    "fit_dist = vi_collections[vis_s_i].offset_dists[0]\n",
    "\n",
    "# Scales\n",
    "true_vls = true_subj_models[vis_s_i]['mdl'].scales[0]\n",
    "fit_dist = vi_collections[vis_s_i].scale_dists[0]\n",
    "\n",
    "# Direct mappings\n",
    "#true_vls = true_subj_models[vis_s_i]['mdl'].direct_mappings[0]\n",
    "#fit_dist = vi_collections[vis_s_i].direct_mapping_dists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_neuron_locs = true_subj_models[vis_s_i]['u_neuron_locs']\n",
    "vis_true_offsets = true_vls.detach().numpy()\n",
    "vis_fit_offsets = fit_dist.dists[0](vis_neuron_locs).detach().numpy()\n",
    "vis_neuron_locs = 1000*np.concatenate([vis_neuron_locs.numpy(), np.zeros([vis_neuron_locs.shape[0], 1])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_image = generate_dot_image_3d(image_shape=[1001, 1001, 1], dot_ctrs=vis_neuron_locs, \n",
    "                                   dot_vls=vis_true_offsets, \n",
    "                     ellipse_shape=e_shape) \n",
    "\n",
    "fit_image = generate_dot_image_3d(image_shape=[1001, 1001, 1], dot_ctrs=vis_neuron_locs, dot_vls=vis_fit_offsets, \n",
    "                     ellipse_shape=e_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.squeeze(true_image))\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.squeeze(fit_image))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at true and estimated distributions over modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_m = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn a transformation to align modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_neuron_locs = true_subj_models[0]['u_neuron_locs']\n",
    "true_u_modes = true_u_dists(u_neuron_locs).detach().cpu().numpy()\n",
    "est_u_modes = prior_collection.u_dists[0](u_neuron_locs).detach().cpu().numpy()\n",
    "#est_u_modes = vi_collections[0].u_dists[0](u_neuron_locs).detach().cpu().numpy()\n",
    "u_neuron_locs = 1000*np.concatenate([u_neuron_locs.numpy(), np.zeros([u_neuron_locs.shape[0], 1])], axis=1)\n",
    "\n",
    "p_neuron_locs = true_subj_models[0]['p_neuron_locs']\n",
    "true_p_modes = true_p_dists(p_neuron_locs).detach().cpu().numpy()\n",
    "#true_p_modes = true_subj_models[0]['mdl'].p[0].detach().cpu().numpy()\n",
    "est_p_modes = prior_collection.p_dists[0](p_neuron_locs).detach().cpu().numpy()\n",
    "#est_p_modes = vi_collections[0].p_dists[0](p_neuron_locs).detach().cpu().numpy()\n",
    "#est_p_modes = vi_collections[0].s_mdl.p[0].detach().cpu().numpy()\n",
    "p_neuron_locs = 1000*np.concatenate([p_neuron_locs.numpy(), np.zeros([p_neuron_locs.shape[0], 1])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_t = np.linalg.lstsq(est_u_modes, true_u_modes, rcond=None)\n",
    "mode_t = mode_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_u_modes_t = np.matmul(est_u_modes, mode_t)\n",
    "est_p_modes_t = np.matmul(est_p_modes, np.linalg.inv(mode_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_p_modes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_shape = [21, 21, 1]\n",
    "\n",
    "true_u_image = generate_dot_image_3d(image_shape=[1001, 1001, 1], dot_ctrs=u_neuron_locs, \n",
    "                                   dot_vls=true_u_modes[:,vis_m], \n",
    "                                   ellipse_shape=e_shape) \n",
    "\n",
    "fit_u_image = generate_dot_image_3d(image_shape=[1001, 1001, 1], dot_ctrs=u_neuron_locs, \n",
    "                                  dot_vls=est_u_modes_t[:,vis_m], \n",
    "                                   ellipse_shape=e_shape) \n",
    "\n",
    "true_p_image = generate_dot_image_3d(image_shape=[1002, 1002, 1], dot_ctrs=p_neuron_locs, \n",
    "                                   dot_vls=true_p_modes[:,vis_m], \n",
    "                                   ellipse_shape=e_shape) \n",
    "\n",
    "fit_p_image = generate_dot_image_3d(image_shape=[1002, 1002, 1], dot_ctrs=p_neuron_locs, \n",
    "                                  dot_vls=est_p_modes[:,vis_m], \n",
    "                                  ellipse_shape=e_shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.squeeze(true_u_image), clim=[0, 1])\n",
    "plt.colorbar()\n",
    "plt.title('True u mode')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.squeeze(fit_u_image), clim=[0, 1])\n",
    "plt.title('Est u mode')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.squeeze(true_p_image), clim=[0, 1])\n",
    "plt.colorbar()\n",
    "plt.title('True p mode')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.squeeze(fit_p_image), clim=[0, 1.4])\n",
    "plt.title('Est p mode')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
